{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719337d5",
   "metadata": {
    "id": "719337d5"
   },
   "source": [
    "# Prompt Engineering Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c4ed4",
   "metadata": {
    "id": "923c4ed4"
   },
   "source": [
    "Submitted by,\n",
    "> Suhail Chand <br>\n",
    "> suhail.chand@outlook.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c7229",
   "metadata": {
    "id": "557c7229"
   },
   "source": [
    "This notebook demonstrates some Prompt Engineering techniques with the **Llama 2 open-source LLM.** We will try to cover some of the major ideas behind Prompt Engineering, and in the process of examining various examples and their outputs, observe the importance of various prompt elements to the LLM's abilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c450637",
   "metadata": {
    "id": "1c450637"
   },
   "source": [
    "## Importing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711ac10",
   "metadata": {
    "id": "c711ac10"
   },
   "source": [
    "We will use the 5-bit integer quantized version of the Llama2 13b chat model on a single T4 GPU in Google Colab. To load the model, we shall install all the pre-requisites and download the model weights from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e865907",
   "metadata": {
    "id": "6e865907"
   },
   "source": [
    "### Enabling NVIDIA cuBLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f2d70",
   "metadata": {
    "id": "d78f2d70"
   },
   "source": [
    "**About the cuBLAS library**\n",
    "\n",
    "- The cuBLAS library provides highly optimized implementations of common linear algebra routines, such as matrix-matrix multiplication, matrix-vector multiplication, and linear system solvers. These operations are crucial in many scientific and computational tasks, including machine learning, numerical simulations, and data analysis.\n",
    "\n",
    "- By utilizing the parallel processing power of GPUs, cuBLAS can significantly accelerate linear algebra computations compared to traditional CPU-based implementations. GPUs are designed with many cores and specialized hardware for parallel computation, making them well-suited for performing large-scale matrix operations in parallel.\n",
    "\n",
    "- ***Applications that involve extensive matrix computations, such as deep learning models, numerical simulations, and scientific computations, can benefit from using the cuBLAS library***. It enables faster and more efficient calculations, reducing the overall computational time and enabling researchers and developers to tackle more complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db434a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9db434a7",
    "outputId": "66ae73b8-4fa5-4837-8b0d-c8b735d7880f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/9.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m248.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m280.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m229.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28  --force-reinstall --upgrade --no-cache-dir -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b8dc9",
   "metadata": {
    "id": "9e3b8dc9"
   },
   "source": [
    "**!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"**:\n",
    "- This line sets an environment variable named CMAKE_ARGS with the value -DLLAMA_CUBLAS=on.\n",
    "- It indicates that the cuBLAS library should be enabled for GPU-accelerated linear algebra operations performed by the \"llama-cpp-python\" package, if available.\n",
    "\n",
    "**FORCE_CMAKE=1**:\n",
    "- This line sets an environment variable called FORCE_CMAKE with the value 1.\n",
    "- It instructs the installation process to use the CMake build system.\n",
    "\n",
    "**pip install llama-cpp-python**: This command uses the pip package manager to install or upgrade the llama-cpp-python package.\n",
    "\n",
    "**--force-reinstall**: This option forces the reinstallation of the package, even if it is already installed on the system.\n",
    "\n",
    "**--upgrade**: This option ensures that the installed package is upgraded to the latest version if a newer version is available.\n",
    "\n",
    "**--no-cache-dir**: This option disables the use of pip's cache directory, which means that the package will be downloaded and installed directly from the source without using any cached files.\n",
    "\n",
    "**--verbose**: This option enables verbose output during the installation process, providing more detailed information about each step being performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206e3ce",
   "metadata": {
    "id": "a206e3ce"
   },
   "source": [
    "### Downloading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2ec44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeb2ec44",
    "outputId": "cb17ae31-6333-47f4-f612-a61de65a4674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "# For downloading the models from Hugging Face\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2e3269",
   "metadata": {
    "id": "cf2e3269"
   },
   "outputs": [],
   "source": [
    "# Llama class from the llama_cpp library\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# hf_hub_download function from the Hugging Face Hub library\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ff807",
   "metadata": {
    "id": "835ff807"
   },
   "source": [
    "- Llama2 is a collection of pretrained and fine-tuned LLMs ranging from 7 billion to 70 billion parameters.\n",
    "- GGUF (Generalized Graph Unification Format) is a format introduced by the llama.cpp, an extensible, future-proof format which stores more information about the model as metadata.\n",
    "- It also includes significantly improved tokenization code, including for the first time full support for special tokens. This has been shown to improve performance, especially with models that use new special tokens and implement custom prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08998faa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "2772ea4178f94e9cbf62218792c3add7",
      "fb4b50e46c7544e9b11ebdd914afe906",
      "0660804e21fe4cce852c83936b485840",
      "b028711e8da94196bc330e21868d090e",
      "f568f12d80914c04a8fbea7e131d8ebc",
      "b13a4568b089447fad4897daa1f96c88",
      "3ebff24533a7423994d5e3fdc5f2a7b0",
      "aa73b09eea324b5697c64a5969c88eaf",
      "9c6461f2bce04f659b8609a104abcf82",
      "c63ad3b44ec04bf487f5d28ee8bec8a8",
      "a922c74398ba4da2863761dd23ae2099"
     ]
    },
    "id": "08998faa",
    "outputId": "78e13709-2e2f-4cd7-fd0f-de9c81cd42de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772ea4178f94e9cbf62218792c3add7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    filename=\"llama-2-13b-chat.Q5_K_M.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6ebec",
   "metadata": {
    "id": "b5a6ebec"
   },
   "source": [
    "**hf_hub_download()**:\n",
    "- The hf_hub_download() function will connect to the Hugging Face Model Hub, locate the specified model using the provided repo_id, and then download it.\n",
    "- The downloaded model will be saved locally with the specified filename.\n",
    "- The model_path variable will contain the path to the downloaded model file on your local file system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437d07d",
   "metadata": {
    "id": "e437d07d"
   },
   "source": [
    "### Initialising the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545fe26e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "545fe26e",
    "outputId": "3ef3c514-359c-46d3-fe3a-4a755fd484e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llama_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,                # CPU cores.\n",
    "    n_batch=512,                # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43,            # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096,                 # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ba597",
   "metadata": {
    "id": "371ba597"
   },
   "source": [
    "**model_path**: Specifies the path to the model.\n",
    "\n",
    "**n_threads**:\n",
    "- Specifies the number of CPU cores that the Llama instance should use for its operations.\n",
    "- Modern CPUs often have multiple cores, allowing them to perform multiple tasks simultaneously. By specifying the number of threads, you're controlling how many of these cores the Llama instance will utilize.\n",
    "\n",
    "**n_batch**:\n",
    "- It's used to specify the batch size for processing data with the model.\n",
    "- It should be between 1 and n_ctx (context window), taking into consideration the GPU's VRAM capacity.\n",
    "- In machine learning, especially with deep learning models, data is often processed in batches rather than one sample at a time. This is because it can lead to more efficient computations, taking advantage of parallel processing capabilities in modern hardware.\n",
    "- In this case, n_batch=512 means that the Llama instance is set to process data in batches of 512.\n",
    "- Choosing an appropriate batch size is important for balancing computational efficiency with memory constraints.\n",
    "\n",
    "**n_gpu_layers**: Specifies the number of GPU layers (adjust based on GPU VRAM).\n",
    "\n",
    "**n_ctx**:\n",
    "- This parameter specifies the context window size, which is a crucial aspect of how a language model processes and generates text.\n",
    "- In the context of language models, a \"context window\" refers to the range of tokens (words or subwords) that the model considers when generating responses or making predictions.\n",
    "- When the model generates text, it doesn't consider the entire input sequence but focuses on a limited window of context. This helps the model manage memory and computational resources efficiently.\n",
    "- A larger context window can provide more context for generating coherent and contextually relevant responses. However, it may also come with increased memory requirements, as the model needs to store and process a larger amount of text.\n",
    "- The choice of n_ctx should strike a balance between the need for context and the available computational resources (both CPU and GPU memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519c781",
   "metadata": {
    "id": "0519c781"
   },
   "source": [
    "## Prompt Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3309ab67",
   "metadata": {
    "id": "3309ab67"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \"\"\"\n",
    "\n",
    "def generate_prompt(system_message, user_input):\n",
    "    prompt = prompt_template.format(system_message=system_message, user_message=user_input)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ed29cd",
   "metadata": {
    "id": "43ed29cd"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"Respond to the user question based on the user prompt.\"\"\"\n",
    "\n",
    "def generate_llama_response(user_input):\n",
    "    # Generate prompt from user_input and system_message\n",
    "    prompt=generate_prompt(system_message, user_input)\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = llama_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc92b20",
   "metadata": {
    "id": "8cc92b20"
   },
   "source": [
    "- **max_tokens**: Specifies the maximum number of *tokens* that the model should generate in response to the prompt.\n",
    "- **temperature**: Controls the *randomness* of the generated response. A higher temperature value will result in a more random response, while a lower temperature value will result in a more predictable response. In this case, it's set to 0, which means the response will be as deterministic as possible.\n",
    "- **top_p**: Controls the *diversity* of the generated response. A higher value of top_p will result in a more diverse response, while a lower value will result in a less diverse response. In this case, it's set to 0.95, which means the model will try to generate a diverse response.\n",
    "- **repeat_penalty**: Controls the *penalty for repeating tokens* in the generated response. A higher value of repeat_penalty will result in a lower probability of repeating tokens, while a lower value will result in a higher probability of repeating tokens. In this case, it's set to 1.2, which means the model will try to avoid repeating tokens.\n",
    "- **top_k**: Controls the *maximum number* of tokens that will be considered when generating the response. In this case, it's set to 50, which means the model will consider up to 50 tokens when generating the response.\n",
    "- **stop**: List of tokens that should *stop the generation* of response. In this case, it's set to ['INST'], which means the model will stop generating tokens when it encounters the token \"INST\".\n",
    "- **echo**: Controls whether the prompt should be echoed back to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc150760",
   "metadata": {
    "id": "dc150760"
   },
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ec5f5",
   "metadata": {
    "id": "735ec5f5"
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825dc366",
   "metadata": {
    "id": "825dc366"
   },
   "source": [
    "Prompt templates consist of two main components: fixed text and variable slots. These components work together to provide a structured framework for generating prompts in generative AI tasks.\n",
    "\n",
    "1. Fixed Text:\n",
    "    - Fixed text refers to the static or pre-defined portions of the prompt template.\n",
    "    - It can include any text, instructions, or context that remains constant across different prompts generated using the template.\n",
    "    - Fixed text helps set the overall tone, style, or specific requirements for the generated output.\n",
    "\n",
    "2. Variable Slots:\n",
    "    - Variable slots are the dynamic parts of the prompt template that can be filled with different values or options.\n",
    "    - These slots act as placeholders for specific information that can vary from prompt to prompt. Often the variable slots are used to gather input from the users.\n",
    "    - In this way, these slots allow for customization and flexibility in generating prompts that vary by user input.\n",
    "    - A variable slot is usually delimited by a specific character (e.g., triple backticks ``` ) so that this portion of the prompt can be dynamically altered when the input is given.\n",
    "\n",
    "Fixed and variable slots in a prompt are arranged in a prompt template as the system message, few shot examples and user messages. Prompt templates can be repeatedly reused by changing the user message. The system message is included at the beginning of the prompt and is used to prime the model with context, instructions, or other information relevant to the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55602f",
   "metadata": {
    "id": "9c55602f"
   },
   "source": [
    "### 1. Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2bd6d",
   "metadata": {
    "id": "8dd2bd6d"
   },
   "source": [
    "- The key idea behind zero-shot prompts is that the model can leverage its general language understanding and knowledge to generate relevant responses, even for tasks it has not been explicitly trained on.\n",
    "- This is achieved by providing the model with a prompt that includes a description or instruction about the desired task or topic, without any additional training data specific to that task.\n",
    "- Zero-shot prompts are useful in scenarios where training data for a specific task is limited or unavailable.\n",
    "- They allow for more flexible and versatile use of generative AI models, as they can generate responses for a wide range of tasks without the need for task-specific training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55872142",
   "metadata": {
    "id": "55872142"
   },
   "source": [
    "**Classify customer review in the inputs as positive or negative in sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac462ef",
   "metadata": {
    "id": "cac462ef"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "Classify customer reviews in the input as positive or negative in sentiment.\n",
    "Reviews will be delimited by triple backticks, that is, ```.\n",
    "Do not explain your answer. Your answer should only contain the label: positive or negative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7768d6",
   "metadata": {
    "id": "ec7768d6"
   },
   "outputs": [],
   "source": [
    "zero_shot_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \"\"\"\n",
    "\n",
    "def generate_prompt(system_message, user_input):\n",
    "    prompt = zero_shot_template.format(system_message=system_message, user_message=user_input)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b76933",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99b76933",
    "outputId": "8e243b99-eae3-4da2-a092-53ffbd4e0468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Positive\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\n",
    "I couldn't be happier with my experience at your store!\n",
    "The staff went above and beyond to assist me, providing exceptional customer service.\n",
    "They were friendly, knowledgeable, and genuinely eager to help.\n",
    "The product I purchased exceeded my expectations and was exactly what I was looking for.\n",
    "From start to finish, everything was seamless and enjoyable.\n",
    "I will definitely be returning and recommending your store to all my friends and family.\n",
    "Thank you for making my shopping experience so wonderful!\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(customer_review)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e3c0cf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e3c0cf6",
    "outputId": "8a374b1d-5678-4249-c2e4-5de0652591e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Negative\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\"\n",
    "I am extremely disappointed with the service I received at your store!\n",
    "The staff was rude and unhelpful, showing no regard for my concerns.\n",
    "Not only did they ignore my requests for assistance, but they also had the audacity to speak to me condescendingly.\n",
    "It's clear that your company values profit over customer satisfaction.\n",
    "I will never shop here again and will make sure to spread the word about my awful experience.\n",
    "You've lost a loyal customer, and I hope others steer clear of your establishment!\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(customer_review)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d021b",
   "metadata": {
    "id": "7f2d021b"
   },
   "source": [
    "**Entity extraction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9727fedf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9727fedf",
    "outputId": "8d979f7a-f95b-447b-f7db-6679ed7e88b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are the entities extracted from the customer review:\n",
      "\n",
      "* Brand: Logitech\n",
      "* Product: Laser Gaming Mouse\n",
      "* Features: weight, feel, precision, button placement, size, shape\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"Your task is act like assistant and extract entities from customer reviews in the input.\n",
    "Reviews will be delimited by triple backticks, that is, ```.\n",
    "Do not explain your answer.\"\"\"\n",
    "\n",
    "customer_review = \"\"\"I had a old but very nice logitech lazer gamin mouse, my dog at the cord off it so had to get a replacement.\n",
    "I was tempted to get another logitech because well I knew it was a sure thing.\n",
    "Anyways I saw the reviews on this mouse and thought it looked awesome so I thought I would give it a try.\n",
    "Well it does indeed look awesome and feels good in the hand.\n",
    "My old mouse was weighted and kind of like the feel of the heft but I'm pleased with this new one and so long as it doesn't fail on me would say its definitely worth the price.\n",
    "I would have had to play something like a First Person Shooter side by side to get a real idea how they compare on precision but this new mouse seems fine. Again my logitech was probably more than 10 years old so I can't compare to a new one.\n",
    "If I had to guess they based the button placement, size and shape of this mouse off the logitech, don't know.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(customer_review)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e4d26",
   "metadata": {
    "id": "142e4d26"
   },
   "source": [
    "In this scenario, the model identified only three entities, whereas the review mentions four entities. To achieve the desired output, we'll employ Few-Shot Prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba6c9d",
   "metadata": {
    "id": "30ba6c9d"
   },
   "source": [
    "### 2. Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502bdbd",
   "metadata": {
    "id": "1502bdbd"
   },
   "source": [
    "- Few-shot prompts refer to a technique used in generative AI tasks where a model is prompted to generate responses for tasks or topics with only a limited amount of training data.\n",
    "- Unlike zero-shot prompts that require no task-specific training, few-shot prompts provide the model with a small amount of training data to adapt it to a specific task or domain.\n",
    "- The idea behind few-shot prompts is to adapt a pre-trained model using a small number of examples or demonstrations related to the target task. This allows the model to observe task-specific patterns and improve its performance on that particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682a2b9",
   "metadata": {
    "id": "a682a2b9"
   },
   "source": [
    "**Extract relevant entities from customer reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daaa8c04",
   "metadata": {
    "id": "daaa8c04"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"Your task is act like assistant and extract entities from customer review as done for below examples.\n",
    "Reviews will be delimited by triple backticks, that is, ```.\n",
    "Do not explain your answer.\"\"\"\n",
    "\n",
    "review_1= \"\"\"Ordered grey which advertises green lighting, when you're going for a cheap aesthetic, it's upsetting. Mouse works fine.\"\"\"\n",
    "assistant_1 =\"\"\"Entities: [Mouse, Logitech MX Master, DPI Buttons, Mouse Wheel, Wire]\"\"\"\n",
    "\n",
    "review_2=\"\"\"\n",
    "I bought one of these for PC gaming. Loved it, then bought another for work.\n",
    "This mouse is not on par with high end mouses from like the Logitech MX Master series, but at 1/5-/8th the price, I didn't expect that level of quality.\n",
    "It does perform well, mouse wheel feels weighty, side buttons are well place with different textures so you can tell them apart.\n",
    "DPI buttons are handy for adjusting between games, work jobs, etc.\n",
    "The mouse does feel rather plasticky and cheap, but for the money, it about what I expected.\n",
    "I like a wired mouse to avoid the pointer/game jumping around due to latency.\n",
    "Long wire too, so snagging issues are minimized. Great value overall.\"\"\"\n",
    "assistant_2 = \"\"\"Entities: [Mouse, Logitech MX Master, DPI Buttons, Mouse Wheel, Wire]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7cd466e",
   "metadata": {
    "id": "e7cd466e"
   },
   "outputs": [],
   "source": [
    "new_review = \"\"\"I had a old but very nice logitech lazer gamin mouse, my dog at the cord off it so had to get a replacement.\n",
    "I was tempted to get another logitech because well I knew it was a sure thing.\n",
    "Anyways I saw the reviews on this mouse and thought it looked awesome so I thought I would give it a try.\n",
    "Well it does indeed look awesome and feels good in the hand.\n",
    "My old mouse was weighted and kind of like the feel of the heft but I'm pleased with this new one and so long as it doesn't fail on me would say its definitely worth the price.\n",
    "I would have had to play something like a First Person Shooter side by side to get a real idea how they compare on precision but this new mouse seems fine. Again my logitech was probably more than 10 years old so I can't compare to a new one.\n",
    "If I had to guess they based the button placement, size and shape of this mouse off the logitech, don't know.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cba27",
   "metadata": {
    "id": "fa3cba27"
   },
   "source": [
    "The few shot prompt extends the few shot example list by adding the user input for which we need a completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1f5c26",
   "metadata": {
    "id": "2f1f5c26"
   },
   "outputs": [],
   "source": [
    "first_turn_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \\n{assistant_message}\\n</s> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa42f5a",
   "metadata": {
    "id": "8aa42f5a"
   },
   "outputs": [],
   "source": [
    "examples_template = \"\"\"<s>[INST]\\n ```{user_message}``` \\n [/INST] \\n {assistant_message}\\n</s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de03fc3",
   "metadata": {
    "id": "3de03fc3"
   },
   "outputs": [],
   "source": [
    "prediction_template = \"\"\"<s>[INST]\\n ```{user_message}```[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bd5383",
   "metadata": {
    "id": "b0bd5383"
   },
   "outputs": [],
   "source": [
    "first_example = first_turn_template.format(system_message=system_message,user_message=review_1,assistant_message=assistant_1)\n",
    "examples=examples_template.format(user_message=review_2,assistant_message=assistant_2)\n",
    "few_shot_examples  =first_example + examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a203a69",
   "metadata": {
    "id": "2a203a69"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(few_shot_examples,new_input):\n",
    "    prompt = few_shot_examples + prediction_template.format(user_message=new_input)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c65da8",
   "metadata": {
    "id": "c5c65da8"
   },
   "outputs": [],
   "source": [
    "def generate_llama_response(user_input):\n",
    "    # Generate prompt\n",
    "    prompt=generate_prompt(few_shot_examples, user_input)\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = llama_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb55ba2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bb55ba2",
    "outputId": "c1a64705-f513-42df-df8e-d4cdf44561b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Entities: [Mouse, Logitech Laser Gaming Mouse, First Person Shooter]\n"
     ]
    }
   ],
   "source": [
    "response = generate_llama_response(new_review)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f30b9",
   "metadata": {
    "id": "941f30b9"
   },
   "source": [
    "- Few-shot prompts are particularly useful when there is a scarcity of task-specific training data or when adapting a pre-trained model to a new domain or task.\n",
    "- By providing a small amount of relevant training data, the model can quickly learn and generalize from the examples, improving its performance on the target task.\n",
    "- It's important to note that the effectiveness of few-shot prompts depends on the quality and representativeness of the training data provided. While few-shot learning can yield promising results, it may not always match the performance of models trained with a larger amount of task-specific data.\n",
    "- Overall, few-shot prompts offer a middle ground between zero-shot prompts and fully trained models, allowing for targeted adaptation and improved performance on specific tasks with limited training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c9b2d",
   "metadata": {
    "id": "505c9b2d"
   },
   "source": [
    "### 3. Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd45b0",
   "metadata": {
    "id": "35cd45b0"
   },
   "source": [
    "- Chain-of-Thought prompting is a technique used in generative AI tasks to guide the model's response generation by providing a sequence of related prompts or questions.\n",
    "- Instead of a single prompt, a CoT consists of multiple interconnected steps that build upon each other to guide the model's thinking process. These steps represent the \"thinking\" process that we want the model to follow.\n",
    "- The purpose of CoT prompting is to encourage the model to generate more coherent and contextually relevant responses by guiding its thought process in a structured manner.\n",
    "- Each step in the chain serves as a stepping stone, providing additional context or constraints for the model to consider while generating the response.\n",
    "- CoT prompts could also be augmented with few-shot examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226cff8",
   "metadata": {
    "id": "7226cff8"
   },
   "source": [
    "**Generate a detailed key information from each customer complaint, presented in a structured JSON format by following a logical sequence of acknowledging the issue, explaining the cause for customer complains.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b940c68d",
   "metadata": {
    "id": "b940c68d"
   },
   "outputs": [],
   "source": [
    "cot_prompt_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe1ed6f7",
   "metadata": {
    "id": "fe1ed6f7"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an assistant that helps a customer service representatives from a mobile phone company to better understand customer complaints.\n",
    "Customer complaints will be submitted as text delimited by triple backticks, that is, ```.\n",
    "For each complaint, extract the following information and present it only in a JSON format:\n",
    "1. phone_model: This is the name of the phone - if unknown, just say “UNKNOWN”\n",
    "2. phone_price: The price in dollars - if unknown, assume it to be 1000 $\n",
    "3. complaint_desc: A short description/summary of the complaint in less than 20 words\n",
    "4. additional_charges: How much in dollars did the customer spend to fix the problem? - this should be an integer\n",
    "5. refund_expected: TRUE or FALSE - check if the customer explicitly mentioned the word “refund” to tag as TRUE. If unknown, assume that the customer is not expecting a refund\n",
    "\n",
    "Take a step-by-step approach in your response, and give a detailed explanation before sharing your final answer in the following JSON format:\n",
    "{phone_model:, phone_price:, complaint_desc:, additional_charges:, refund_expected:}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdaf8139",
   "metadata": {
    "id": "cdaf8139"
   },
   "outputs": [],
   "source": [
    "customer_complaint = \"\"\"\n",
    "I am fuming with anger and regret over my purchase of the XUI890.\n",
    "First, the price tag itself was exorbitant at 1500 $, making me expect exceptional quality.\n",
    "Instead, it turned out to be a colossal disappointment.\n",
    "The additional charges to fix its constant glitches and defects drained my wallet even more.\n",
    "I spend 275 $ to get a new battery.\n",
    "The final straw was when the phone's camera malfunctioned, and the repair cost was astronomical.\n",
    "I demand a full refund and an apology for this abysmal product.\n",
    "Returning it would be a relief, as this phone has become nothing but a money pit. Beware, fellow buyers!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "830d1465",
   "metadata": {
    "id": "830d1465"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(system_message, user_input):\n",
    "    prompt=cot_prompt_template.format(system_message=system_message, user_message=user_input)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac4dfda9",
   "metadata": {
    "id": "ac4dfda9"
   },
   "outputs": [],
   "source": [
    "def generate_llama_response(user_input):\n",
    "    #generate prompt\n",
    "    prompt=generate_prompt(system_message,user_input)\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = llama_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47471355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47471355",
    "outputId": "b914ae27-d94e-4b9a-86bb-357acde290d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you with that! Here's the information extracted from the customer complaint:\n",
      "\n",
      "1. Phone model: XUI890\n",
      "2. Phone price: 1500 $ (mentioned in the complaint as \"exorbitant\")\n",
      "3. Complaint desc: Constant glitches and defects, camera malfunctioned\n",
      "4. Additional charges: 275 $ (mentioned for a new battery)\n",
      "5. Refund expected: TRUE (explicitly mentioned in the complaint as \"demand a full refund\")\n",
      "\n",
      "Here's the final answer in JSON format:\n",
      "{phone_model:\"XUI890\", phone_price:1500, complaint_desc:\"Constant glitches and defects, camera malfunctioned\", additional_charges:275, refund_expected:TRUE}\n"
     ]
    }
   ],
   "source": [
    "response = generate_llama_response(customer_complaint)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af3d10",
   "metadata": {
    "id": "88af3d10"
   },
   "source": [
    "- Chain of thought prompting helps in maintaining coherence and relevance in the generated responses by providing a structured framework for the model's thinking process.\n",
    "- It ensures that the model considers the necessary context and constraints while generating each part of the response.\n",
    "- By guiding the model's thought process through a chain of interconnected prompts, chain of thought prompting can lead to more focused, contextually appropriate, and coherent responses in generative AI tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0761be7",
   "metadata": {
    "id": "f0761be7"
   },
   "source": [
    "Sometimes portions of the instruction might not be followed. To avoid such a situation, we should add a reiteration of the task summary at the end of the system message like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75fba658",
   "metadata": {
    "id": "75fba658"
   },
   "outputs": [],
   "source": [
    "system_message =\"\"\"\n",
    "You are an assistant that helps a customer service representatives from a mobile phone company to better understand customer complaints.\n",
    "For each complaint, extract the following information:\n",
    "1. phone_model: This is the name of the phone - if unknown, just say “UNKNOWN”\n",
    "2. phone_price: The price in dollars - if unknown, assume it to be 1000 $\n",
    "3. complaint_desc: A short description/summary of the complaint in less than 20 words\n",
    "4. additional_charges: How much in dollars did the customer spend to fix the problem? - this should be an integer\n",
    "5. refund_expected: TRUE or FALSE - check if the customer explicitly mentioned the word “refund” to tag as TRUE. If unknown, assume that the customer is not expecting a refund\n",
    "\n",
    "Take a step-by-step approach in your response, and give a detailed explanation before sharing your final answer in the following JSON format:\n",
    "{phone_model:, phone_price:, complaint_desc:, additional_charges:, refund_expected:}.\n",
    "\n",
    "To reiterate, explain your rationale in detail before presenting  your final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46133215",
   "metadata": {
    "id": "46133215"
   },
   "outputs": [],
   "source": [
    "customer_complaint = \"\"\"\n",
    "I am fuming with anger and regret over my purchase of the XUI890.\n",
    "First, the price tag itself was exorbitant at 1500 $, making me expect exceptional quality.\n",
    "Instead, it turned out to be a colossal disappointment.\n",
    "The additional charges to fix its constant glitches and defects drained my wallet even more.\n",
    "I spend 275 $ to get a new battery.\n",
    "The final straw was when the phone's camera malfunctioned, and the repair cost was astronomical.\n",
    "I demand a full refund and an apology for this abysmal product.\n",
    "Returning it would be a relief, as this phone has become nothing but a money pit. Beware, fellow buyers!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eea8b351",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eea8b351",
    "outputId": "809ac7a6-fb97-40ea-8c0a-79573468fc7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you with that! Here's my step-by-step approach in understanding the customer complaint:\n",
      "\n",
      "Step 1: Identify the phone model\n",
      "The customer mentions \"XUI890\" as the name of their phone. Therefore, we can confirm that the phone model is XUI890.\n",
      "\n",
      "Step 2: Determine the price of the phone\n",
      "The customer states that the price tag was exorbitant at $1500, which suggests that the phone was a high-end device. However, since they did not explicitly mention the price, we can only assume it to be $1000 as a default value.\n",
      "\n",
      "Step 3: Summarize the complaint\n",
      "The customer is unhappy with their purchase of the XUI890 due to its constant glitches and defects. They had to spend additional money for repairs, including $275 for a new battery, and the camera malfunctioned eventually. The customer demands a full refund and an apology.\n",
      "\n",
      "Step 4: Determine additional charges\n",
      "The customer mentions spending $275 for a new battery, but there is no information about other repair costs. Therefore, we can only assume that the additional charges are $275.\n",
      "\n",
      "Step 5: Check if refund is expected\n",
      "The customer explicitly states \"I demand a full refund,\" which suggests that they expect a refund. Therefore, we can confirm that the refund is expected to be TRUE.\n",
      "\n",
      "Now, let me present my final answer in JSON format based on the information gathered above:\n",
      "{phone_model:\"XUI890\", phone_price:$1000, complaint_desc:\"Constant glitches and defects, camera malfunctioned\", additional_charges:$275, refund_expected:TRUE}.\n"
     ]
    }
   ],
   "source": [
    "response = generate_llama_response(customer_complaint)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884c9c0",
   "metadata": {
    "id": "1884c9c0"
   },
   "source": [
    "### 4. Self-consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916418f7",
   "metadata": {
    "id": "916418f7"
   },
   "source": [
    "#### Simple Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b815f3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b815f3f4",
    "outputId": "e218ef0d-1ba6-4a15-c566-566bb1a88825"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Answer (Adjusted Recipe):\n",
      "- 2 eggs\n",
      "- 1 teaspoon of baking powder\n",
      "- 1/2 teaspoon of salt\n",
      "- 1 cup of milk\n",
      "- 1 teaspoon of vanilla extract\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat oven to 375 degrees Fahrenheit (190 degrees Celsius).\n",
      "2. In a large mixing bowl, whisk together flour and sugar.\n",
      "3. Add in softened butter and mix until crumbly.\n",
      "4. Beat in eggs and vanilla extract until well combined.\n",
      "5. Gradually add in milk while continuously stirring to avoid lumps.\n",
      "6. Pour the batter into a greased 9x13-inch baking dish.\n",
      "7. Bake for 20-25 minutes or until a toothpick inserted comes out clean.\n",
      "8. Remove from oven and let cool before serving.\n",
      "\n",
      "Adjusted Recipe (Serves 10):\n",
      "\n",
      "* Multiply all ingredients by 2.5 (except milk, which remains the same)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat oven to 375 degrees Fahrenheit (190 degrees Celsius).\n",
      "2. In a large mixing bowl, whisk together flour and sugar.\n",
      "3. Add in softened butter and mix until crumbly.\n",
      "4. Beat in eggs and vanilla extract until well combined.\n",
      "5. Gradually add in milk while continuously stirring to avoid lumps.\n",
      "6. Pour the batter into a greased 12x18-inch baking dish.\n",
      "7. Bake for 30-35 minutes or until a toothpick inserted comes out clean.\n",
      "8. Remove from oven and let cool before serving.\n"
     ]
    }
   ],
   "source": [
    "# Simple prompting to adjust recipe ingredients\n",
    "simple_prompt = \"\"\"\n",
    "Adjust the following recipe to serve 10 people.\n",
    "Original Recipe (Serves 4):\n",
    "- 2 cups of flour\n",
    "- 1 cup of sugar\n",
    "- 0.5 cup of butter\n",
    "\"\"\"\n",
    "\n",
    "response = llama_llm(\n",
    "    prompt=simple_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "simple_answer = response[\"choices\"][0][\"text\"]\n",
    "print(\"Simple Answer (Adjusted Recipe):\")\n",
    "print(simple_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d929d",
   "metadata": {
    "id": "803d929d"
   },
   "source": [
    "- The output generated through simple prompting presents a recipe that is clear and straightforward, making it easy for beginners to follow.\n",
    "- However, it falls short in several critical areas. Notably, the lack of detailed calculations for adjusting ingredient quantities based on the desired increase in servings may leave learners confused about the scaling process.\n",
    "- Additionally, there is no explanation provided on how to adjust these quantities, limiting their understanding of the underlying mathematical principles involved in recipe scaling.\n",
    "- Furthermore, the inclusion of unnecessary ingredients and instructional details not relevant to the task contributes to the phenomenon of \"hallucination\" in the output, which can further mislead learners and detract from the overall quality of the recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23507da0",
   "metadata": {
    "id": "23507da0"
   },
   "source": [
    "#### Self-consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6d096",
   "metadata": {
    "id": "f6a6d096"
   },
   "source": [
    "In self-consistency, we generate multiple answers to the same question and pick the answer that is repeated the most across these occurrences. This is particularly valuable for factual questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "824232d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824232d6",
    "outputId": "1d1edd67-41c3-4bd0-c165-460a5667d8e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answers:\n",
      " Sure, I'd be happy to help! Here are five different adjustments to the original recipe to serve 10 people:\n",
      "\n",
      "Adjustment 1 (Classic Portion):\n",
      "\n",
      "* Flour: 4 cups (x2 = 8 cups for 10 servings)\n",
      "* Sugar: 2 cups (x2 = 4 cups for 10 servings)\n",
      "* Butter: 1 cup (x2 = 2 cups for 10 servings)\n",
      "\n",
      "Adjustment 2 (Increased Portion):\n",
      "\n",
      "* Flour: 6 cups (x3 = 18 cups for 10 servings)\n",
      "* Sugar: 3 cups (x3 = 9 cups for 10 servings)\n",
      "* Butter: 2 cups (x3 = 6 cups for 10 servings)\n",
      "\n",
      "Adjustment 3 (Small Portion):\n",
      "\n",
      "* Flour: 2.5 cups (x1.5 = 5 cups for 10 servings)\n",
      "* Sugar: 1.5 cups (x1.5 = 6 cups for 10 servings)\n",
      "* Butter: 1 cup (x1.5 = 3 cups for 10 servings)\n",
      "\n",
      "Adjustment 4 (Vegan Version):\n",
      "\n",
      "* Flour: 2.5 cups (x1.5 = 5 cups for 10 servings)\n",
      "* Sugar: 1 cup (x1.5 = 6 cups for 10 servings)\n",
      "* Butter Substitute: 1/2 cup (x1.5 = 3 cups for 10 servings)\n",
      "\n",
      "Adjustment 5 (Gluten-Free Version):\n",
      "\n",
      "* Flour: 2 cups of gluten-free all-purpose flour (x2 = 4 cups for 10 servings)\n",
      "* Sugar: 1 cup (x2 = 4 cups for 10 servings)\n",
      "* Butter Substitute: 1/2 cup (x2 = 4 cups for 10 servings)\n",
      "\n",
      "I hope these adjustments help you make the perfect batch of cookies for your group size!\n"
     ]
    }
   ],
   "source": [
    "# System message to provide context for the LLM\n",
    "system_message = \"\"\"\n",
    "You are a cooking assistant. Adjust recipe ingredient quantities based on desired servings.\n",
    "\"\"\"\n",
    "\n",
    "# Template for generating multiple answers\n",
    "answers_template = \"\"\"\n",
    "The original recipe serves 4 people:\n",
    "- 2 cups of flour\n",
    "- 1 cup of sugar\n",
    "- 0.5 cup of butter\n",
    "\n",
    "Adjust the following recipe to serve 10 people. Provide {num_answers} different adjustments.\n",
    "\"\"\"\n",
    "\n",
    "# Formatting the prompt to generate multiple answers (Self-Consistency)\n",
    "answers_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=answers_template.format(\n",
    "        num_answers=5  # Generate 5 distinct answers\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sending the request to generate multiple answers\n",
    "response = llama_llm(\n",
    "    prompt=answers_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False  # Do not return the prompt\n",
    ")\n",
    "\n",
    "# Extracting the generated answers\n",
    "factual_answers = response[\"choices\"][0][\"text\"]\n",
    "print(\"Generated Answers:\")\n",
    "print(factual_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9329a4ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9329a4ad",
    "outputId": "0c5caa7a-dded-4aaa-c39d-cc63868b1831"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer (Self-Consistency):\n",
      " Sure, I'd be happy to help! Based on the five different adjustments provided, here are the most frequently suggested quantities for each ingredient for a batch of cookies that serves 10 people:\n",
      "\n",
      "Adjustment 3 (Small Portion):\n",
      "\n",
      "* Flour: 2.5 cups (x1.5 = 5 cups for 10 servings)\n",
      "* Sugar: 1.5 cups (x1.5 = 6 cups for 10 servings)\n",
      "* Butter: 1 cup (x1.5 = 3 cups for 10 servings)\n",
      "\n",
      "These quantities are the most frequently suggested in the five adjustments provided, and they will yield a batch of cookies that serves 10 people with a smaller portion size.\n"
     ]
    }
   ],
   "source": [
    "# Template for selecting the most frequent answer\n",
    "consistency_template = \"\"\"\n",
    "Here are {num_answers} answers to the recipe adjustment:\n",
    "Answers:\n",
    "{answers}\n",
    "\n",
    "Choose the most frequently suggested quantities for each ingredient.\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Formatting the prompt to select the most frequent answer\n",
    "consistency_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=consistency_template.format(\n",
    "        num_answers=5,  # Number of answers generated previously\n",
    "        answers=factual_answers  # Pass the generated answers from the previous step\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sending the request to select the most frequent answer\n",
    "response = llama_llm(\n",
    "    prompt=consistency_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False  # Do not return the prompt\n",
    ")\n",
    "\n",
    "# Extracting and printing the final answer\n",
    "final_answer = response[\"choices\"][0][\"text\"]\n",
    "print(\"Final Answer (Self-Consistency):\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090c5d0",
   "metadata": {
    "id": "2090c5d0"
   },
   "source": [
    "- The self-consistency prompting approach yields an output that is more tailored to the specific needs of the recipe, addressing some of the issues present in the simple prompting method.\n",
    "- This output maintains a degree of consistency in ingredient adjustments and offers multiple perspectives by showcasing different methods for recipe scaling, catering to various preferences. However, it still contains inconsistencies in ingredient amounts for the same serving size, leading to potential confusion regarding measurements like sugar and flour.\n",
    "- Additionally, the output lacks clear explanations for why certain quantities were selected over others, leaving users uncertain about which method to trust. This ambiguity could undermine the confidence of learners who are trying to grasp the intricacies of recipe adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7352ac",
   "metadata": {
    "id": "4a7352ac"
   },
   "source": [
    "### 5. Tree-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07c16b",
   "metadata": {
    "id": "9f07c16b"
   },
   "source": [
    "Tree-of-thought prompting is a generalization of chain-of-thought prompting where the model is prompted to take multiple reasoning paths. This forces the LLM into a deliberate reasoning mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37757bb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37757bb1",
    "outputId": "fd6ca752-506b-40b1-b6fa-69daf2ef0d2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Solutions (Tree of Thought):\n",
      " Sure, I'd be happy to help! Here are five possible solutions for calculating the ingredient adjustments for the recipe to serve 10 people:\n",
      "\n",
      "Solution 1: Calculate the adjustment factor mathematically\n",
      "\n",
      "To calculate the adjustment factor, we divide the desired number of servings (10) by the original number of servings (4):\n",
      "\n",
      "Adjustment Factor = 10 / 4 = 2.5\n",
      "\n",
      "Now, we can use this adjustment factor to adjust each ingredient quantity:\n",
      "\n",
      "Solution 2: Adjust each ingredient using the adjustment factor\n",
      "\n",
      "* Flour: 2 cups x 2.5 = 5 cups\n",
      "* Sugar: 1 cup x 2.5 = 2.5 cups\n",
      "* Butter: 0.5 cup x 2.5 = 1.25 cups\n",
      "\n",
      "Solution 3: Present the adjusted quantities in bullet points\n",
      "\n",
      "Here are the adjusted ingredient quantities for the recipe to serve 10 people:\n",
      "\n",
      "* Flour: 5 cups\n",
      "* Sugar: 2.5 cups\n",
      "* Butter: 1.25 cups\n",
      "\n",
      "Solution 4: Use a formula to adjust the ingredient quantities\n",
      "\n",
      "We can use the following formula to adjust each ingredient quantity:\n",
      "\n",
      "New Quantity = Original Quantity x (Desired Servings / Original Servings)\n",
      "\n",
      "Using this formula, we can calculate the adjusted ingredient quantities as follows:\n",
      "\n",
      "* Flour: New Quantity = 2 cups x (10 / 4) = 5 cups\n",
      "* Sugar: New Quantity = 1 cup x (10 / 4) = 2.5 cups\n",
      "* Butter: New Quantity = 0.5 cups x (10 / 4) = 1.25 cups\n",
      "\n",
      "Solution 5: Use a recipe adjustment chart\n",
      "\n",
      "Here is a recipe adjustment chart that shows the adjusted ingredient quantities for the recipe to serve 10 people:\n",
      "\n",
      "| Original Quantity | Adjusted Quantity |\n",
      "| --- | --- |\n",
      "| Flour | 5 cups |\n",
      "| Sugar | 2.5 cups |\n",
      "| Butter | 1.25 cups |\n",
      "\n",
      "To use this chart, simply find the original quantity of each ingredient on the left side of the chart, and then look across to find the adjusted quantity for the desired number of servings (10).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tree of Thought Prompting to generate multiple solutions\n",
    "tot_prompt_template = \"\"\"\n",
    "Generate {num_solutions} possible solutions for the following problem:\n",
    "Problem:\n",
    "Calculate the ingredient adjustments for the following recipe:\n",
    "Original Recipe (Serves 4):\n",
    "- 2 cups of flour\n",
    "- 1 cup of sugar\n",
    "- 0.5 cup of butter\n",
    "Desired Servings: 10 people.\n",
    "\n",
    "1. Calculate the adjustment factor mathematically: (10 servings / 4 servings).\n",
    "2. Adjust each ingredient using the adjustment factor.\n",
    "3. Present the adjusted quantities in bullet points.\n",
    "\"\"\"\n",
    "\n",
    "tot_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=tot_prompt_template.format(\n",
    "        num_solutions=5  # Generate 5 distinct solutions\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sending the request to the LLM for generating solutions\n",
    "response = llama_llm(\n",
    "    prompt=tot_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    echo=False  # Do not return the prompt\n",
    ")\n",
    "\n",
    "# Extracting and printing generated solutions\n",
    "tot_solutions = response[\"choices\"][0][\"text\"]\n",
    "print(\"Generated Solutions (Tree of Thought):\")\n",
    "print(tot_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ba55ad0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ba55ad0",
    "outputId": "af6c7609-91a1-454b-a19b-bb61158cf68a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Solutions:\n",
      " As a cooking assistant, I will evaluate each solution based on accuracy and mathematical reasoning.\n",
      "\n",
      "Solution 1: Calculate the adjustment factor mathematically\n",
      "Strengths: This solution is simple and straightforward, using basic multiplication to calculate the adjustment factor. It also highlights the concept of scaling up or down a recipe based on the desired number of servings.\n",
      "\n",
      "Weaknesses: The solution does not provide any visual representation of the adjusted quantities, which can make it more difficult to understand and apply the adjustments.\n",
      "\n",
      "Solution 2: Adjust each ingredient using the adjustment factor\n",
      "Strengths: This solution provides a clear and concise way of adjusting each ingredient quantity based on the adjustment factor. It also shows how the adjustment factor is applied to each ingredient.\n",
      "\n",
      "Weaknesses: The solution does not provide any visual representation of the adjusted quantities, and it may be more difficult to understand for some users who are not familiar with basic multiplication.\n",
      "\n",
      "Solution 3: Present the adjusted quantities in bullet points\n",
      "Strengths: This solution provides a clear and concise list of the adjusted ingredient quantities, making it easy to understand and apply the adjustments. It also highlights each ingredient individually, which can be helpful for users who want to quickly identify the adjustments.\n",
      "\n",
      "Weaknesses: The solution does not provide any visual representation of the adjusted quantities, and it may be more difficult to understand for some users who prefer to see the calculations.\n",
      "\n",
      "Solution 4: Use a formula to adjust the ingredient quantities\n",
      "Strengths: This solution provides a clear and concise way of adjusting each ingredient quantity based on the desired number of servings. It also highlights the concept of scaling up or down a recipe using a formula.\n",
      "\n",
      "Weaknesses: The solution may be more difficult to understand for some users who are not familiar with basic multiplication or formulas.\n",
      "\n",
      "Solution 5: Use a recipe adjustment chart\n",
      "Strengths: This solution provides a clear and concise way of adjusting each ingredient quantity based on the desired number of servings. It also provides a visual representation of the adjusted quantities, which can be helpful for users who prefer to see the calculations.\n",
      "\n",
      "Weaknesses: The solution may be more difficult to understand for some users who are not familiar with basic multiplication or charts. Additionally, the chart may become cluttered if there are many ingredients or servings to adjust.\n",
      "\n",
      "In conclusion, all of the solutions provide accurate adjustments for the recipe to serve 10 people, but they vary in their strengths and weaknesses based on their presentation and level of mathematical reasoning. Solution 3, which presents the adjusted quantities in bullet points, is a good compromise between accuracy and clarity, while Solution 5, which uses a recipe adjustment chart, provides a clear visual representation of the adjustments but may be more difficult to understand for some users.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluation prompt template\n",
    "evaluation_template = \"\"\"\n",
    "For the following problem: Adjust the recipe to serve 10 people, evaluate each solution based on accuracy and mathematical reasoning:\n",
    "Solutions:\n",
    "{solutions}\n",
    "\n",
    "Present your evaluation of each solution, highlighting strengths and weaknesses.\n",
    "\"\"\"\n",
    "\n",
    "# Create a new prompt to evaluate the solutions\n",
    "evaluation_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=evaluation_template.format(\n",
    "        solutions=tot_solutions\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sending the request to evaluate the solutions\n",
    "response = llama_llm(\n",
    "    prompt=evaluation_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    echo=False  # Do not return the prompt\n",
    ")\n",
    "\n",
    "# Extract and print evaluation results\n",
    "evaluation_results = response[\"choices\"][0][\"text\"]\n",
    "print(\"Evaluation of Solutions:\")\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58aabc62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58aabc62",
    "outputId": "22e4dab1-a720-4bc7-82ed-b64c9665a9c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Solutions:\n",
      " As a cooking assistant, I would rank the solutions as follows:\n",
      "\n",
      "1. Solution 3: Present the adjusted quantities in bullet points\n",
      "\n",
      "This solution provides a clear and concise list of the adjusted ingredient quantities, making it easy to understand and apply the adjustments. It highlights each ingredient individually, which can be helpful for users who want to quickly identify the adjustments. Additionally, this solution does not require any mathematical calculations or formulas, making it accessible to a wide range of users.\n",
      "\n",
      "2. Solution 5: Use a recipe adjustment chart\n",
      "\n",
      "This solution provides a clear visual representation of the adjusted quantities, which can be helpful for users who prefer to see the calculations. However, it may be more difficult to understand for some users who are not familiar with basic multiplication or charts. Additionally, the chart may become cluttered if there are many ingredients or servings to adjust.\n",
      "\n",
      "3. Solution 1: Calculate the adjustment factor mathematically\n",
      "\n",
      "This solution is simple and straightforward, using basic multiplication to calculate the adjustment factor. However, it does not provide any visual representation of the adjusted quantities, which can make it more difficult to understand and apply the adjustments.\n",
      "\n",
      "4. Solution 2: Adjust each ingredient using the adjustment factor\n",
      "\n",
      "This solution provides a clear and concise way of adjusting each ingredient quantity based on the adjustment factor. However, it does not provide any visual representation of the adjusted quantities, and it may be more difficult to understand for some users who are not familiar with basic multiplication.\n",
      "\n",
      "5. Solution 4: Use a formula to adjust the ingredient quantities\n",
      "\n",
      "This solution provides a clear and concise way of adjusting each ingredient quantity based on the desired number of servings. However, it may be more difficult to understand for some users who are not familiar with basic multiplication or formulas.\n",
      "\n",
      "In conclusion, Solution 3, which presents the adjusted quantities in bullet points, is the best solution as it provides a clear and concise list of the adjusted ingredient quantities, making it easy to understand and apply the adjustments. It is accessible to a wide range of users and does not require any mathematical calculations or formulas.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Ranking prompt template\n",
    "ranking_template = \"\"\"\n",
    "For the following problem: Adjust the recipe to serve 10 people, rank the following solutions based on their accuracy and mathematical reasoning:\n",
    "Evaluations:\n",
    "{evaluations}\n",
    "\n",
    "Rank the solutions and explain which one is the best and why.\n",
    "\"\"\"\n",
    "\n",
    "# Create a new prompt to rank the solutions\n",
    "ranking_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=ranking_template.format(\n",
    "        evaluations=evaluation_results\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sending the request to rank the solutions\n",
    "response = llama_llm(\n",
    "    prompt=ranking_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    echo=False  # Do not return the prompt\n",
    ")\n",
    "\n",
    "# Extract and print the ranked solutions\n",
    "ranked_solutions = response[\"choices\"][0][\"text\"]\n",
    "print(\"Ranked Solutions:\")\n",
    "print(ranked_solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42619b1",
   "metadata": {
    "id": "c42619b1"
   },
   "source": [
    "- The tree-of-thought prompting technique provides comprehensive solutions by breaking down each methodical process for scaling recipes, significantly enhancing understanding of various mathematical approaches involved.\n",
    "- Unlike the self-consistency prompting approach, which presents conflicting ingredient amounts and lacks clarity on selection rationale, the ToT method systematically organizes information, allowing for the evaluation of each solution based on a clear ranking scheme.\n",
    "- By showcasing multiple strategies for achieving the same goal, this output encourages critical thinking and helps in discerning the most effective method for different contexts.\n",
    "- Additionally, the structured nature of the ToT output reduces ambiguity, making it easier to grasp complex concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba7c88",
   "metadata": {
    "id": "86ba7c88"
   },
   "source": [
    "### 6. Rephrase & Respond (RaR) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d143bf7",
   "metadata": {
    "id": "6d143bf7"
   },
   "source": [
    "- The Rephrase and Respond (RaR) prompting technique is a clever way to boost the accuracy and clarity of responses from large language models. Instead of jumping straight into answering a question, the model is first asked to rephrase and expand the original prompt—then respond to it.\n",
    "\n",
    "- This two-step approach helps in a few key ways:\n",
    "    - It clarifies ambiguous or poorly worded questions.\n",
    "    - It encourages the model to think more deeply about the intent behind the query.\n",
    "    - It often leads to more accurate, context-aware answers.\n",
    "- RaR is especially useful in domains where precision matters—like legal, medical, or technical contexts—and can be used in both single-step and two-step formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f19167b7",
   "metadata": {
    "id": "f19167b7"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are tasked to answer queries on financial information.\n",
    "Only answer the specific question presented by the user.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78f87eb8",
   "metadata": {
    "id": "78f87eb8"
   },
   "outputs": [],
   "source": [
    "rephrase_prompt_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "\n",
    "Question:\n",
    "{question}.\n",
    "\n",
    "Observe the context presented above, rephrase and expand the above question to help you do better answering.\n",
    "Maintain all the information in the original question.\n",
    "Please note that you only have to rephrase the question, do not mention the context.\n",
    "The context is only presented for your reference.\n",
    "\n",
    "Present your answer in the following format:\n",
    "Rephrased Question:\n",
    "<rephrased-question-here>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0aa7e",
   "metadata": {
    "id": "2fd0aa7e"
   },
   "source": [
    "Here is an extract from the Tesla 2022 10-K statement that will be used as context for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60200870",
   "metadata": {
    "id": "60200870"
   },
   "outputs": [],
   "source": [
    "tesla_annual_report_context =\"\"\"\n",
    "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
    "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bfa8416",
   "metadata": {
    "id": "7bfa8416"
   },
   "outputs": [],
   "source": [
    "factual_question = \"What was the increase in annual revenue in 2022 compared to 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fee51e9",
   "metadata": {
    "id": "2fee51e9"
   },
   "outputs": [],
   "source": [
    "rephrase_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=rephrase_prompt_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        question=factual_question\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62ef4bda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62ef4bda",
    "outputId": "a39a55da-95ea-40b5-81cc-774d602e4acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      " <<SYS>> \n",
      " \n",
      "You are tasked to answer queries on financial information.\n",
      "Only answer the specific question presented by the user.\n",
      " \n",
      " <</SYS>>```\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "===\n",
      "\n",
      "Question:\n",
      "What was the increase in annual revenue in 2022 compared to 2021?.\n",
      "\n",
      "Observe the context presented above, rephrase and expand the above question to help you do better answering.\n",
      "Maintain all the information in the original question.\n",
      "Please note that you only have to rephrase the question, do not mention the context.\n",
      "The context is only presented for your reference.\n",
      "\n",
      "Present your answer in the following format:\n",
      "Rephrased Question:\n",
      "<rephrased-question-here>\n",
      "``` /n [/INST] \n"
     ]
    }
   ],
   "source": [
    "print(rephrase_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81cfc971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81cfc971",
    "outputId": "473c1c14-05c9-4e59-b6ae-3651f03a9108"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response = llama_llm(\n",
    "    prompt=rephrase_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "781427c1",
   "metadata": {
    "id": "781427c1"
   },
   "outputs": [],
   "source": [
    "rephrased_question = response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9435033",
   "metadata": {
    "id": "b9435033"
   },
   "outputs": [],
   "source": [
    "rephrase_marker = rephrased_question.find('Rephrased Question:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "303f02dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "303f02dd",
    "outputId": "dc72f996-2819-41f1-f10f-2e22ee298920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What was the increase in annual revenues from 2021 to 2022? Specifically, what was the dollar amount of the increase in total revenues during this period?\n"
     ]
    }
   ],
   "source": [
    "print(rephrased_question[rephrase_marker+19:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41b9de4e",
   "metadata": {
    "id": "41b9de4e"
   },
   "outputs": [],
   "source": [
    "rephrased_factual_question = rephrased_question[rephrase_marker+19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a217fec",
   "metadata": {
    "id": "3a217fec"
   },
   "outputs": [],
   "source": [
    "response_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "\n",
    "Original Question:\n",
    "{question}\n",
    "\n",
    "Rephrased Question:\n",
    "{rephrased_question}\n",
    "\n",
    "Given the above context, use your answer for the rephrased question presented above to answer the original question.\n",
    "Present your final answer in the following format.\n",
    "Final Answer: <your-final-answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be39cbdd",
   "metadata": {
    "id": "be39cbdd"
   },
   "outputs": [],
   "source": [
    "response_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=response_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        question=factual_question,\n",
    "        rephrased_question=rephrased_factual_question\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b924fc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b924fc6",
    "outputId": "3c09a6f1-bf39-485d-f626-34f2e7951c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      " <<SYS>> \n",
      " \n",
      "You are tasked to answer queries on financial information.\n",
      "Only answer the specific question presented by the user.\n",
      " \n",
      " <</SYS>>```\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "===\n",
      "\n",
      "Original Question:\n",
      "What was the increase in annual revenue in 2022 compared to 2021?\n",
      "\n",
      "Rephrased Question:\n",
      " What was the increase in annual revenues from 2021 to 2022? Specifically, what was the dollar amount of the increase in total revenues during this period?\n",
      "\n",
      "Given the above context, use your answer for the rephrased question presented above to answer the original question.\n",
      "Present your final answer in the following format.\n",
      "Final Answer: <your-final-answer>\n",
      "``` /n [/INST] \n"
     ]
    }
   ],
   "source": [
    "print(response_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e32f74f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e32f74f",
    "outputId": "aa0e4b78-3921-49ed-b877-71ac25acf53f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response = llama_llm(\n",
    "    prompt=response_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b31812d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b31812d",
    "outputId": "db96727a-ec78-41a2-bca1-59129d89bd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Based on the information provided, the increase in annual revenues from 2021 to 2022 was $27.64 billion. This is calculated by taking the total revenues for 2022 ($81.46 billion) and subtracting the total revenues for 2021.\n",
      "\n",
      "Final Answer: $27.64 billion\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e7fec",
   "metadata": {
    "id": "cd2e7fec"
   },
   "source": [
    "### 7. Chain-of-Verification (CoVe) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd72e3b",
   "metadata": {
    "id": "6cd72e3b"
   },
   "source": [
    "- The Chain-of-Verification (CoVe) prompting technique is a structured method designed to reduce hallucinations in large language models by encouraging self-critique and fact-checking.\n",
    "- This technique is especially effective for complex or factual queries, as it helps the model catch and correct its own mistakes—kind of like proofreading with a built-in fact-checker. It's a powerful tool for improving reliability in AI-generated content.\n",
    "- Instead of relying solely on a single response, CoVe guides the model through a four-step process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed931471",
   "metadata": {
    "id": "ed931471"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are tasked to answer queries on financial information.\n",
    "Only answer the specific question presented by the user.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb17a1",
   "metadata": {
    "id": "0aeb17a1"
   },
   "source": [
    "#### Step 1: Baseline response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bddbdfd",
   "metadata": {
    "id": "1bddbdfd"
   },
   "outputs": [],
   "source": [
    "baseline_prompt_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "\n",
    "Use the above context to answer the following question:\n",
    "Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "356f58b0",
   "metadata": {
    "id": "356f58b0"
   },
   "outputs": [],
   "source": [
    "tesla_annual_report_context =\"\"\"\n",
    "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
    "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a827b1a8",
   "metadata": {
    "id": "a827b1a8"
   },
   "outputs": [],
   "source": [
    "factual_question = \"Which year had more revenue - 2022 or 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a32152d",
   "metadata": {
    "id": "8a32152d"
   },
   "outputs": [],
   "source": [
    "baseline_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=baseline_prompt_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        question=factual_question\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7530a835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7530a835",
    "outputId": "5fbdf526-ad8c-4d86-b792-91d3cce4f5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      " <<SYS>> \n",
      " \n",
      "You are tasked to answer queries on financial information.\n",
      "Only answer the specific question presented by the user.\n",
      " \n",
      " <</SYS>>```\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "===\n",
      "\n",
      "Use the above context to answer the following question:\n",
      "Question:\n",
      "Which year had more revenue - 2022 or 2021?\n",
      "``` /n [/INST] \n"
     ]
    }
   ],
   "source": [
    "print(baseline_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86234685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86234685",
    "outputId": "17795ae2-db68-4f86-e422-27e377363553"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response = llama_llm(\n",
    "    prompt=baseline_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "271c6c56",
   "metadata": {
    "id": "271c6c56"
   },
   "outputs": [],
   "source": [
    "baseline_factual_response = response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d91d6779",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d91d6779",
    "outputId": "f045fa60-9218-4708-ea6e-b6b74264700a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Based on the information provided in the context, we can see that the total revenues for 2022 were $81.46 billion, while there is no specific mention of the total revenues for 2021. Therefore, we can conclude that 2022 had more revenue than 2021.\n"
     ]
    }
   ],
   "source": [
    "print(baseline_factual_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b445ca",
   "metadata": {
    "id": "b8b445ca"
   },
   "source": [
    "#### Step 2: Verification questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "841a594c",
   "metadata": {
    "id": "841a594c"
   },
   "outputs": [],
   "source": [
    "verifications_prompt_template = \"\"\"\n",
    "Your task is to create verification questions based on the below original question and the baseline response.\n",
    "The verification questions are meant for verifying the factual acuracy in the baseline response.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "\n",
    "Question:\n",
    "Use the above context to answer the following question: {question}\n",
    "\n",
    "Baseline Response:\n",
    "{baseline_response}\n",
    "\n",
    "Respond with your verification questions in a JSON format with the following headers.\n",
    "```JSON\n",
    "question1: <verification-question-1>\n",
    "question2: <veriification-question-2>\n",
    "and so on.\n",
    "```\n",
    "Do not provide answers to these verification questions. Respond only with the JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c2a996e",
   "metadata": {
    "id": "0c2a996e"
   },
   "outputs": [],
   "source": [
    "verifications_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=verifications_prompt_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        question=factual_question,\n",
    "        baseline_response=baseline_factual_response\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79f05ea0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79f05ea0",
    "outputId": "9fd959ee-f92f-4884-d66b-5d504d9d0f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      " <<SYS>> \n",
      " \n",
      "You are tasked to answer queries on financial information.\n",
      "Only answer the specific question presented by the user.\n",
      " \n",
      " <</SYS>>```\n",
      "Your task is to create verification questions based on the below original question and the baseline response.\n",
      "The verification questions are meant for verifying the factual acuracy in the baseline response.\n",
      "\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "===\n",
      "\n",
      "Question:\n",
      "Use the above context to answer the following question: Which year had more revenue - 2022 or 2021?\n",
      "\n",
      "Baseline Response:\n",
      "Sure! Based on the information provided in the context, we can see that the total revenues for 2022 were $81.46 billion, while there is no specific mention of the total revenues for 2021. Therefore, we can conclude that 2022 had more revenue than 2021.\n",
      "\n",
      "Respond with your verification questions in a JSON format with the following headers.\n",
      "```JSON\n",
      "question1: <verification-question-1>\n",
      "question2: <veriification-question-2>\n",
      "and so on.\n",
      "```\n",
      "Do not provide answers to these verification questions. Respond only with the JSON.\n",
      "``` /n [/INST] \n"
     ]
    }
   ],
   "source": [
    "print(verifications_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3a0d0a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3a0d0a1",
    "outputId": "0f5e1be2-0d67-4d59-cb0b-c9268d243d65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response = llama_llm(\n",
    "    prompt=verifications_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf0531b1",
   "metadata": {
    "id": "bf0531b1"
   },
   "outputs": [],
   "source": [
    "verification_factual_questions = response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3522e39c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3522e39c",
    "outputId": "efb31e76-3c55-443d-9900-5f09854551e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three verification questions based on the provided context and baseline response:\n",
      "\n",
      "{\n",
      "\"question1\": \"What was the total revenue for 2021 according to the given context?\",\n",
      "\"question2\": \"How much did total revenues increase from 2021 to 2022, as stated in the context?\",\n",
      "\"question3\": \"Does the baseline response accurately state that there is no specific mention of total revenues for 2021 in the given context?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(verification_factual_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff095bf",
   "metadata": {
    "id": "2ff095bf"
   },
   "source": [
    "#### Step 3: Verification responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d812b1de",
   "metadata": {
    "id": "d812b1de"
   },
   "outputs": [],
   "source": [
    "verification_responses_template = \"\"\"Answer the following question correctly based on the context given below:\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "\n",
    "Question: {verification_question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b047137",
   "metadata": {
    "id": "6b047137"
   },
   "outputs": [],
   "source": [
    "verification_question_beginning = verification_factual_questions.find(\"{\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e15b423",
   "metadata": {
    "id": "0e15b423"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "verification_factual_questions_dict = json.loads(verification_factual_questions[verification_question_beginning:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6cc22f5f",
   "metadata": {
    "id": "6cc22f5f"
   },
   "outputs": [],
   "source": [
    "verification_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54fe424b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54fe424b",
    "outputId": "ca4e2f1d-ac21-4ee3-d6a7-60ea4dfe5a95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['What was the total revenue for 2021 according to the given context?', 'How much did total revenues increase from 2021 to 2022, as stated in the context?', 'Does the baseline response accurately state that there is no specific mention of total revenues for 2021 in the given context?'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification_factual_questions_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5fe4444d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fe4444d",
    "outputId": "fc75c77e-9e7b-44e2-ebd1-34e0364f9834"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "for verification_factual_question in verification_factual_questions_dict.values():\n",
    "\n",
    "    verification_responses_prompt = prompt_template.format(\n",
    "        system_message=system_message,\n",
    "        user_message=verification_responses_template.format(\n",
    "            context=tesla_annual_report_context,\n",
    "            verification_question=verification_factual_question\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = llama_llm(\n",
    "        prompt=verification_responses_prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    verification_responses.append(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3d37d07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3d37d07",
    "outputId": "1342bb93-a1c7-4d13-e856-3238e5fa6261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the total revenue for 2021 according to the given context?\n",
      "Sure! Based on the context provided, the total revenue for 2021 is $81.46 billion.\n",
      "How much did total revenues increase from 2021 to 2022, as stated in the context?\n",
      "Sure! Based on the context you provided, total revenues increased by $27.64 billion from 2021 to 2022.\n",
      "Does the baseline response accurately state that there is no specific mention of total revenues for 2021 in the given context?\n",
      "Yes, the baseline response accurately states that there is no specific mention of total revenues for 2021 in the given context. The text only mentions total revenues for 2022 and the increase compared to the prior year, but does not provide any information about total revenues for 2021.\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(verification_factual_questions_dict.values(), verification_responses):\n",
    "    print(q)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "931b759f",
   "metadata": {
    "id": "931b759f"
   },
   "outputs": [],
   "source": [
    "verification_factual_question_answer_pairs = ''\n",
    "\n",
    "for q, a in zip(verification_factual_questions_dict.values(), verification_responses):\n",
    "    verification_factual_question_answer_pairs += ('\\n' + \\\n",
    "    f\"Verification Question: {q}\" + '\\n' + \\\n",
    "    f\"Response: {a}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "112299a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "112299a9",
    "outputId": "92c257b1-f615-4cfe-e7ef-342e718c631f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Question: What was the total revenue for 2021 according to the given context?\n",
      "Response: Sure! Based on the context provided, the total revenue for 2021 is $81.46 billion.\n",
      "Verification Question: How much did total revenues increase from 2021 to 2022, as stated in the context?\n",
      "Response: Sure! Based on the context you provided, total revenues increased by $27.64 billion from 2021 to 2022.\n",
      "Verification Question: Does the baseline response accurately state that there is no specific mention of total revenues for 2021 in the given context?\n",
      "Response: Yes, the baseline response accurately states that there is no specific mention of total revenues for 2021 in the given context. The text only mentions total revenues for 2022 and the increase compared to the prior year, but does not provide any information about total revenues for 2021.\n"
     ]
    }
   ],
   "source": [
    "print(verification_factual_question_answer_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a19bda5",
   "metadata": {
    "id": "8a19bda5"
   },
   "source": [
    "#### Step 4: Final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f6f0bc6",
   "metadata": {
    "id": "7f6f0bc6"
   },
   "outputs": [],
   "source": [
    "final_prompt_template = \"\"\"\n",
    "Given the below `Context`, `Original Query` and `Baseline Answer`, analyze the `Verification Questions & Answers` to finally filter the refined answer.\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "Original Query: {original_question}\n",
    "Baseline Answer: {baseline_response}\n",
    "\n",
    "Verification Questions & Answer Pairs:\n",
    "{verification_question_answer_pairs}\n",
    "\n",
    "To reiterate, you should answer the original query accounting for the veracity of the verification question answer pairs and the context.\n",
    "Only answer the original question and do not present any additional details. Present your final answer in the following format:\n",
    "Final Refined Answer:<your-final-answer>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f08d8b17",
   "metadata": {
    "id": "f08d8b17"
   },
   "outputs": [],
   "source": [
    "final_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=final_prompt_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        original_question=factual_question,\n",
    "        baseline_response=baseline_factual_response,\n",
    "        verification_question_answer_pairs=verification_factual_question_answer_pairs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7da0af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7da0af1",
    "outputId": "bdc8e8ea-7279-4a34-bfea-0e95e98bde81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      " <<SYS>> \n",
      " \n",
      "You are tasked to answer queries on financial information.\n",
      "Only answer the specific question presented by the user.\n",
      " \n",
      " <</SYS>>```\n",
      "Given the below `Context`, `Original Query` and `Baseline Answer`, analyze the `Verification Questions & Answers` to finally filter the refined answer.\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "===\n",
      "Original Query: Which year had more revenue - 2022 or 2021?\n",
      "Baseline Answer: Sure! Based on the information provided in the context, we can see that the total revenues for 2022 were $81.46 billion, while there is no specific mention of the total revenues for 2021. Therefore, we can conclude that 2022 had more revenue than 2021.\n",
      "\n",
      "Verification Questions & Answer Pairs:\n",
      "\n",
      "Verification Question: What was the total revenue for 2021 according to the given context?\n",
      "Response: Sure! Based on the context provided, the total revenue for 2021 is $81.46 billion.\n",
      "Verification Question: How much did total revenues increase from 2021 to 2022, as stated in the context?\n",
      "Response: Sure! Based on the context you provided, total revenues increased by $27.64 billion from 2021 to 2022.\n",
      "Verification Question: Does the baseline response accurately state that there is no specific mention of total revenues for 2021 in the given context?\n",
      "Response: Yes, the baseline response accurately states that there is no specific mention of total revenues for 2021 in the given context. The text only mentions total revenues for 2022 and the increase compared to the prior year, but does not provide any information about total revenues for 2021.\n",
      "\n",
      "To reiterate, you should answer the original query accounting for the veracity of the verification question answer pairs and the context.\n",
      "Only answer the original question and do not present any additional details. Present your final answer in the following format:\n",
      "Final Refined Answer:<your-final-answer>``` /n [/INST] \n"
     ]
    }
   ],
   "source": [
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec39aad4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec39aad4",
    "outputId": "3808db5f-e863-45e6-8f0a-94edc6f3a7c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response = llama_llm(\n",
    "    prompt=final_prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    echo=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3641798",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3641798",
    "outputId": "6f2da1b9-9f69-4a7c-bca3-d05c069551d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Refined Answer: Based on the provided context, the total revenue for 2021 is $81.46 billion.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d53b1",
   "metadata": {
    "id": "ff0d53b1"
   },
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "iTRNoRXqZM98",
   "metadata": {
    "id": "iTRNoRXqZM98"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\\n <<SYS>> \\n {system_message} \\n <</SYS>>```{user_message}``` /n [/INST] \"\"\"\n",
    "\n",
    "def generate_prompt(system_message, user_input):\n",
    "    prompt = prompt_template.format(system_message=system_message, user_message=user_input)\n",
    "    return prompt\n",
    "\n",
    "system_message = \"\"\"Respond to the user question based on the user prompt.\"\"\"\n",
    "\n",
    "def generate_llama_response(user_input):\n",
    "    # Generate prompt from user_input and system_message\n",
    "    prompt=generate_prompt(system_message, user_input)\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = llama_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760c87b",
   "metadata": {
    "id": "4760c87b"
   },
   "source": [
    "### 1. Clear and specific instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138b11d",
   "metadata": {
    "id": "0138b11d"
   },
   "source": [
    "- Vague inputs will always give you generic and vague outputs.\n",
    "- The more detailed you are with the context, the better the chance you will get an output that is tailored to your needs.\n",
    "\n",
    "**Industry Setting: E-commerce**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efb60f",
   "metadata": {
    "id": "59efb60f"
   },
   "source": [
    "#### Baseline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "056fed56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "056fed56",
    "outputId": "33144b03-6240-4cf2-8b5b-f9ad6c77d39b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are many great products available in various categories, and what constitutes a \"good\" product can depend on individual preferences and needs. However, here are some popular and highly-regarded products across different categories that you may find useful:\n",
      "\n",
      "1. Electronics:\n",
      "\t* Smartphones: Apple iPhones, Samsung Galaxy series, Google Pixel phones\n",
      "\t* Laptops: MacBook Air/Pro, Dell XPS, HP Envy\n",
      "\t* TVs: OLED TVs from LG, Sony Bravia, Samsung QLED\n",
      "2. Home and Kitchen:\n",
      "\t* Smart home devices: Amazon Echo, Google Nest Hub, Apple HomePod\n",
      "\t* Coffee makers: Keurig, Nespresso, Breville\n",
      "\t* Stand mixers: KitchenAid, Bosch, Anker\n",
      "3. Beauty and Personal Care:\n",
      "\t* Skincare products: Neutrogena Hydrating Facial Cleanser, Olay Regenerist Micro-Sculpting Cream, La Roche-Posay Toleriane Ultra Fluid\n",
      "\t* Makeup: MAC Pro Longwear Foundation, NARS Radiant Creamy Concealer, Urban Decay All Nighter Setting Spray\n",
      "4. Clothing and Accessories:\n",
      "\t* Denim jeans: Levi's 501, Gap Classic Skinny Jeans, Madewell High-Rise Skinny Jeans\n",
      "\t* Activewear: Lululemon leggings, Nike Air Max trainers, Adidas Ultraboost running shoes\n",
      "\t* Handbags: Chanel Classic Flap Bag, Hermès Birkin Bag, Gucci Dionysus Bag\n",
      "5. Health and Wellness:\n",
      "\t* Multivitamins: Garden of Life RAW One for Men/Women, Nature Made Multi for Him/Her, Centrum Performance Women's/Men's\n",
      "\t* Protein powder: Optimum Nutrition Gold Standard 100% Whey, MusclePharm Combat Powder, Vega Sport Performance Protein\n",
      "6. Outdoor and Sports:\n",
      "\t* Hiking boots: Merrell Moab 2 Mid Waterproof, Salomon Quest 4D GTX, Keen Targhee II\n",
      "\t* Running shoes: Nike Air Zoom Pegasus 37, Brooks Ghost 13, Asics Gel-Kayano 27\n",
      "\t* Camping gear: Coleman Evanston 4, REI Co-op Base Camp 6, MSR Elixir 2\n",
      "\n",
      "These are just a few examples of popular and highly-regarded products in each category. It's important to do your own research and read reviews before making a purchase to ensure you find the best product for your specific needs and preferences.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"What are good products?\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96706098",
   "metadata": {
    "id": "96706098"
   },
   "source": [
    "#### Improved Prompt - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81559ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81559ee4",
    "outputId": "b577c313-e1ec-4c5c-8e10-b221c4f77150"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are the top 5 best-selling electronics on Amazon's e-commerce site this month based on sales rank and customer reviews:\n",
      "\n",
      "1. Apple AirPods Pro with Wireless Charging Case (2nd Generation) - This wireless earbuds has a sales rank of #1 in the Electronics category and an average rating of 4.8 out of 5 stars from over 3,000 customer reviews. It's currently priced at $239.99.\n",
      "2. Samsung Galaxy Note 20 Ultra 5G Smartphone - This smartphone has a sales rank of #2 in the Electronics category and an average rating of 4.6 out of 5 stars from over 1,000 customer reviews. It's currently priced at $999.99.\n",
      "3. Amazon Echo (4th Generation) - This smart speaker has a sales rank of #3 in the Electronics category and an average rating of 4.7 out of 5 stars from over 1,000 customer reviews. It's currently priced at $99.99.\n",
      "4. Sony WH-1000XM4 Wireless Noise Canceling Headphones - This wireless headphone has a sales rank of #4 in the Electronics category and an average rating of 4.7 out of 5 stars from over 2,000 customer reviews. It's currently priced at $348.\n",
      "5. Apple Watch Series 6 (GPS + Cellular) - This smartwatch has a sales rank of #5 in the Electronics category and an average rating of 4.7 out of 5 stars from over 1,000 customer reviews. It's currently priced at $399.\n",
      "\n",
      "Please note that these rankings and prices are subject to change and may vary based on your location and availability.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"List the top 5 best-selling electronics on amazon e-commerce site this month\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91a9ba",
   "metadata": {
    "id": "cc91a9ba"
   },
   "source": [
    "#### Improved Prompt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "603ca857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "603ca857",
    "outputId": "26d3c3bb-f878-4ff4-b06b-721a7510ad60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's a detailed comparison of the top 5 best-selling electronics on your e-commerce site this month:\n",
      "\n",
      "1. **Apple AirPods Pro**\n",
      "\t* Features: Active noise cancellation, water resistance, wireless charging case, sweat and water resistant, up to 24 hours battery life (with charging case)\n",
      "\t* Price: $249\n",
      "\t* Customer Ratings: 4.8/5 stars (based on over 10,000 reviews)\n",
      "\t* Why they're popular: The AirPods Pro offer excellent sound quality, long battery life, and innovative features like active noise cancellation and water resistance, making them a top choice for Apple fans and wireless earbuds enthusiasts.\n",
      "2. **Samsung Galaxy S21**\n",
      "\t* Features: 6.2-inch AMOLED display, up to 16GB RAM, up to 512GB storage, quad camera setup with 50MP primary sensor, long battery life (up to 48 hours), fast charging support\n",
      "\t* Price: $799 (for the base model)\n",
      "\t* Customer Ratings: 4.6/5 stars (based on over 1,000 reviews)\n",
      "\t* Why they're popular: The Galaxy S21 offers top-of-the-line specs and features at an affordable price point, making it a great value for those looking for a high-end smartphone without breaking the bank.\n",
      "3. **Amazon Echo Show 8**\n",
      "\t* Features: 8-inch touchscreen display, Alexa voice assistant, built-in camera, dual speakers with Dolby Atmos support, Zigbee hub compatibility\n",
      "\t* Price: $129\n",
      "\t* Customer Ratings: 4.5/5 stars (based on over 300 reviews)\n",
      "\t* Why they're popular: The Echo Show 8 offers a great balance of features and price, providing users with a compact and versatile smart display that can handle everything from video calls to cooking recipes to controlling their smart home devices.\n",
      "4. **Sony WH-1000XM4 Wireless Noise Canceling Headphones**\n",
      "\t* Features: Industry-leading noise cancelation, up to 32 hours battery life (with charging case), quick charging support, NFC pairing, and voice assistant integration with Google Assistant or Alexa\n",
      "\t* Price: $349\n",
      "\t* Customer Ratings: 4.7/5 stars (based on over 100 reviews)\n",
      "\t* Why they're popular: The WH-1000XM4 headphones offer exceptional noise cancelation and battery life, making them a top choice for commuters and music lovers who want high-quality sound without any distractions.\n",
      "5. **Ring Video Doorbell Pro**\n",
      "\t* Features: 1080p video resolution, motion detection, night vision, two-way talk, built-in Alexa support, and customizable alerts and notifications\n",
      "\t* Price: $249\n",
      "\t* Customer Ratings: 4.5/5 stars (based on over 3,000 reviews)\n",
      "\t* Why they're popular: The Ring Video Doorbell Pro offers advanced features like motion detection and night vision at an affordable price point, making it a great choice for those looking to upgrade their home security without breaking the bank.\n",
      "\n",
      "Overall, these top-selling electronics offer a mix of innovative technology, high-quality performance, and competitive pricing that appeals to a wide range of customers. Whether you're an Apple fan, a smartphone enthusiast, or just looking for a great value, there's something on this list for everyone!\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"Provide a detailed comparison of the top 5 best-selling electronics on our e-commerce site this month, including features, prices, and customer ratings.\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c9242",
   "metadata": {
    "id": "8d5c9242"
   },
   "source": [
    "### 2. Keep it clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc1687",
   "metadata": {
    "id": "30bc1687"
   },
   "source": [
    "- Avoid Prompt Injections by using delimiters to specify sections of a prompt.\n",
    "\n",
    "**Industry Setting: Legal Services**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565f7b",
   "metadata": {
    "id": "07565f7b"
   },
   "source": [
    "#### Baseline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c1e6f92b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1e6f92b",
    "outputId": "a0926a9a-0956-4145-92f6-b08522307691"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When drafting a contract, it's important to include several key elements to ensure that the agreement is legally binding and protects the interests of all parties involved. Here are some essential items to include in your contract:\n",
      "\n",
      "1. Parties Involved: Start by clearly identifying the names and addresses of both parties entering into the contract, as well as their roles and responsibilities.\n",
      "2. Scope of Work: Describe the services or goods being provided, including any specific details such as quantity, quality standards, delivery dates, and payment terms.\n",
      "3. Consideration: Specify what each party is providing in exchange for the other party's promises, such as money, goods, or services.\n",
      "4. Termination Clause: Include a provision that outlines how the contract can be terminated, including any notice periods, termination fees, and dispute resolution procedures.\n",
      "5. Payment Terms: Define when and how payments will be made, including the amount, payment frequency, and any late payment penalties or interest charges.\n",
      "6. Warranties and Representations: Specify any promises or guarantees made by one party to the other, such as product quality standards or performance expectations.\n",
      "7. Confidentiality: Protect sensitive information shared between the parties with a confidentiality clause that outlines how it must be handled and disclosed.\n",
      "8. Dispute Resolution: Establish a process for resolving disputes that may arise, such as mediation or arbitration, and specify which state's laws will govern the contract.\n",
      "9. Governing Law: Choose the jurisdiction whose laws will govern the interpretation and enforcement of the contract.\n",
      "10. Signatures: Have both parties sign the contract to indicate their acceptance and agreement to its terms.\n",
      "\n",
      "Remember that this is not an exhaustive list, and you may need to include additional provisions depending on your specific situation. It's always a good idea to consult with a legal professional to ensure your contract accurately reflects your intentions and protects your interests.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"What do I need to include in a contract?\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee577a66",
   "metadata": {
    "id": "ee577a66"
   },
   "source": [
    "#### Improved Prompt - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e29bf08a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e29bf08a",
    "outputId": "d445d1a8-b615-4477-dab1-c91c93bca0cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Here are the essential components of a legally binding contract:\n",
      "\n",
      "1. Offer and Acceptance: There must be a clear offer made by one party, and an unequivocal acceptance by the other party. The terms of the offer and acceptance must be definite and certain.\n",
      "2. Intention to Create Legal Relations: Both parties must have the intention to enter into a legally binding agreement. In other words, they must intend to create a contract that is enforceable in court.\n",
      "3. Consideration: There must be some form of consideration, which means that one party must give something of value to the other party. This can take many forms, such as money, goods, or services. The consideration must be sufficient but need not be adequate.\n",
      "4. Capacity to Contract: Both parties must have the legal capacity to enter into a contract. This means that they must be of legal age and of sound mind.\n",
      "5. Free Consent: Both parties must give their free consent to the terms of the contract. This means that they cannot be forced or coerced into entering into the contract, and they must have a genuine choice about whether or not to agree to its terms.\n",
      "6. Legality of Object: The purpose of the contract must be legal. In other words, the contract cannot involve any illegal activities or purposes.\n",
      "7. Possibility of Performance: The parties must be able to perform their obligations under the contract. If one party is unable to fulfill their promises, the contract may become voidable.\n",
      "8. Written Documentation: While an oral contract can be legally binding in some cases, it is generally advisable to put agreements in writing to avoid misunderstandings and disputes. The written document should clearly outline the terms of the agreement, including any conditions, representations, and warranties.\n",
      "9. Signature: Both parties must sign the contract to indicate their acceptance of its terms. In some cases, a signature may not be required if the party has authorized someone else to enter into the contract on their behalf.\n",
      "10. Legal formalities: Depending on the jurisdiction and the nature of the contract, there may be certain legal formalities that must be observed. For example, in some countries, contracts for the sale of land or real property must be in writing and signed by both parties.\n",
      "\n",
      "These are the essential components of a legally binding contract. It is important to note that different jurisdictions may have slightly different requirements, so it is always advisable to seek legal advice if you are entering into a contract.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Outline the essential components of a legal contract. <What do I need to include in a contract?>\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651109ff",
   "metadata": {
    "id": "651109ff"
   },
   "source": [
    "#### Improved Prompt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e526071",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e526071",
    "outputId": "c5ddd065-ca1d-459c-9765-6e95bdc62022"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Here is a structured list of the necessary elements of a legal contract, based on your user prompt:\n",
      "\n",
      "**Parties Involved:**\n",
      "\n",
      "* The names and addresses of both parties involved in the contract (e.g., \"John Doe\" and \"ABC Corporation\")\n",
      "* A description of each party's role and responsibilities in the agreement\n",
      "\n",
      "**Terms of the Agreement:**\n",
      "\n",
      "* A clear and concise statement of the terms of the agreement, including any specific promises or commitments made by each party (e.g., \"John Doe will provide consulting services to ABC Corporation for a period of one year\")\n",
      "* Any conditions or limitations on the parties' obligations under the contract (e.g., \"The consulting services must be performed during regular business hours only\")\n",
      "\n",
      "**Payment Details:**\n",
      "\n",
      "* The amount and method of payment for the goods or services being provided (e.g., \"ABC Corporation will pay John Doe $10,000 per month for a period of one year, with payments made on the 15th day of each month\")\n",
      "* Any applicable taxes, fees, or other charges associated with the payment (e.g., \"The payment amount includes a 7% sales tax\")\n",
      "\n",
      "**Duration of the Contract:**\n",
      "\n",
      "* The start and end dates of the contract (e.g., \"This agreement shall begin on January 1, 2023, and continue until December 31, 2025\")\n",
      "* Any renewal or termination provisions (e.g., \"Either party may terminate this agreement with 30 days' written notice prior to the end of the initial term or any subsequent renewal period\")\n",
      "\n",
      "**Termination Conditions:**\n",
      "\n",
      "* The circumstances under which the contract can be terminated, including any specific reasons for termination (e.g., \"This agreement may be terminated by either party upon 30 days' written notice in the event of a material breach by the other party\")\n",
      "* Any consequences or penalties associated with termination (e.g., \"In the event of termination, John Doe shall return all confidential information and materials provided by ABC Corporation\")\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions or need additional clarification on any of these elements.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Provide a structured list of the necessary elements of a legal contract in bullet points:\n",
    "<What do I need to include in a contract?>\n",
    "\n",
    "Parties involved\n",
    "Terms of the agreement\n",
    "Payment details\n",
    "Duration of the contract\n",
    "Termination conditions.\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e547b",
   "metadata": {
    "id": "0d9e547b"
   },
   "source": [
    "### 3. Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e84c2",
   "metadata": {
    "id": "473e84c2"
   },
   "source": [
    "- Ask for structured outputs in the form of JSON / Tables.\n",
    "\n",
    "**Industry Setting: Supply Chain Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1853b",
   "metadata": {
    "id": "36b1853b"
   },
   "source": [
    "#### Baseline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c65c3d18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c65c3d18",
    "outputId": "57152bbb-7897-45d8-ab5f-394617e3a5f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here is a list of the inventory items you have listed:\n",
      "\n",
      "1. Laptop Model X\n",
      "2. Smartphone Model Y\n",
      "3. Wireless Headphones\n",
      "4. USB-C Charger\n",
      "5. Bluetooth Speaker\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"List our inventory items\n",
    "\n",
    "Laptop Model X\n",
    "Smartphone Model Y\n",
    "Wireless Headphones\n",
    "USB-C Charger\n",
    "Bluetooth Speaker\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e70ad",
   "metadata": {
    "id": "707e70ad"
   },
   "source": [
    "#### Improved Prompt - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d067429",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d067429",
    "outputId": "bfd343e1-3fcf-4c6a-965a-98f632b12c34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's a JSON report reflecting the current quantities of the inventory items you requested:\n",
      "\n",
      "{\n",
      "\"Laptop Model X\": {\n",
      "\"SKU\": \"LMX-123\",\n",
      "\"Quantity on Hand\": 5,\n",
      "\"Reorder Level\": 10,\n",
      "\"Supplier Name\": \"Acme Laptops\"\n",
      "},\n",
      "\"Smartphone Model Y\": {\n",
      "\"SKU\": \"SMY-456\",\n",
      "\"Quantity on Hand\": 20,\n",
      "\"Reorder Level\": 30,\n",
      "\"Supplier Name\": \"Brightstar Mobile\"\n",
      "},\n",
      "\"Wireless Headphones\": {\n",
      "\"SKU\": \"WH-789\",\n",
      "\"Quantity on Hand\": 15,\n",
      "\"Reorder Level\": 25,\n",
      "\"Supplier Name\": \"AudioXperts\"\n",
      "},\n",
      "\"USB-C Charger\": {\n",
      "\"SKU\": \"UC-CHARGER-001\",\n",
      "\"Quantity on Hand\": 30,\n",
      "\"Reorder Level\": 40,\n",
      "\"Supplier Name\": \"ChargeTech\"\n",
      "},\n",
      "\"Bluetooth Speaker\": {\n",
      "\"SKU\": \"BS-987\",\n",
      "\"Quantity on Hand\": 25,\n",
      "\"Reorder Level\": 35,\n",
      "\"Supplier Name\": \"SpeakerWorld\"\n",
      "}\n",
      "}\n",
      "\n",
      "Please note that the quantities and reorder levels are just examples and can be adjusted based on your actual inventory needs.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"For the following inventory items, generate a JSON report reflecting their current quantities:\n",
    "Laptop Model X\n",
    "Smartphone Model Y\n",
    "Wireless Headphones\n",
    "USB-C Charger\n",
    "Bluetooth Speaker\n",
    "\n",
    "Ensure to include the following fields for each item: Item Name, SKU, Quantity on Hand, Reorder Level, and Supplier Name.\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656eaff",
   "metadata": {
    "id": "b656eaff"
   },
   "source": [
    "#### Improved Prompt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "81998934",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81998934",
    "outputId": "f66b894e-9448-4c7c-f691-9ffd5f0bedc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's the updated inventory report based on typical sales patterns after a sales event:\n",
      "\n",
      "{\n",
      "\"Laptop Model X\": {\n",
      "\"SKU\": \"LMX-123\",\n",
      "\"Updated Quantity on Hand\": 50,\n",
      "\"Reorder Level\": 75\n",
      "},\n",
      "\"Smartphone Model Y\": {\n",
      "\"SKU\": \"SMY-456\",\n",
      "\"Updated Quantity on Hand\": 25,\n",
      "\"Reorder Level\": 50\n",
      "},\n",
      "\"Wireless Headphones\": {\n",
      "\"SKU\": \"WH-789\",\n",
      "\"Updated Quantity on Hand\": 100,\n",
      "\"Reorder Level\": 150\n",
      "},\n",
      "\"USB-C Charger\": {\n",
      "\"SKU\": \"UC-CHARGER\",\n",
      "\"Updated Quantity on Hand\": 20,\n",
      "\"Reorder Level\": 30\n",
      "},\n",
      "\"Bluetooth Speaker\": {\n",
      "\"SKU\": \"BS-102\",\n",
      "\"Updated Quantity on Hand\": 5,\n",
      "\"Reorder Level\": 10\n",
      "}\n",
      "}\n",
      "\n",
      "Here's a breakdown of the updated quantities based on typical sales patterns:\n",
      "\n",
      "* Laptop Model X: Sold out 25 units during the sale event, leaving 50 units in stock. Reorder level is set to 75 units.\n",
      "* Smartphone Model Y: Sold out 15 units during the sale event, leaving 25 units in stock. Reorder level is set to 50 units.\n",
      "* Wireless Headphones: Sold out 30 units during the sale event, leaving 100 units in stock. Reorder level is set to 150 units.\n",
      "* USB-C Charger: Sold out 10 units during the sale event, leaving 20 units in stock. Reorder level is set to 30 units.\n",
      "* Bluetooth Speaker: Sold out all 5 units during the sale event, and are now out of stock. Reorder level is set to 10 units.\n",
      "\n",
      "Note that these quantities are just examples and may vary based on actual sales data and other factors such as lead time for restocking and desired inventory levels.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"For the following inventory items, simulate an updated inventory report after a sales event, reducing the quantities based on typical sales patterns:\n",
    "Laptop Model X\n",
    "Smartphone Model Y\n",
    "Wireless Headphones\n",
    "USB-C Charger\n",
    "Bluetooth Speaker\n",
    "\n",
    "Please provide a JSON report with Item Name, SKU, Updated Quantity on Hand, and Reorder Level.\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0fc45",
   "metadata": {
    "id": "eaf0fc45"
   },
   "source": [
    "#### Other Use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2231b8d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2231b8d4",
    "outputId": "6af10e87-750f-423b-8050-7b599a106a47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are the top 3 played video games on PC in the year 2020 based on data from Steam, the largest digital distribution platform for PC games. The output is in JSON format as requested:\n",
      "\n",
      "[\n",
      "{\n",
      "\"name\": \"PlayerUnknown's Battlegrounds (PUBG)\",\n",
      "\"releaseMonth\": \"March\",\n",
      "\"downloads\": 137.94,\n",
      "\"grossingRevenue\": \"$1.52 billion\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Dota Underlords\",\n",
      "\"releaseMonth\": \"June\",\n",
      "\"downloads\": 86.03,\n",
      "\"grossingRevenue\": \"$179.4 million\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Counter-Strike: Global Offensive (CSGO)\",\n",
      "\"releaseMonth\": \"August\",\n",
      "\"downloads\": 52.32,\n",
      "\"grossingRevenue\": \"$608.8 million\"\n",
      "}\n",
      "]\n",
      "\n",
      "Note that the download figures are in millions and correct to three decimals, as requested. The grossing revenue figures are also included in string format as requested. The games are ordered by descending order of downloads.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Give me the top 3 played video games on PC in the year 2020\n",
    "\n",
    "The output should be in the form of a JSON with\n",
    "1. the game's name (as string),\n",
    "2. release month (as string),\n",
    "3. number of downloads (as a float in millions correct to 3 decimals),\n",
    "4. total grossing revenue (as string)\n",
    "\n",
    "order the games by descending order of downloads\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9636145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9636145",
    "outputId": "38650252-1adf-4c5d-bf84-b9e6122fbd3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! Here are my top three movie recommendations based on your preferences:\n",
      "\n",
      "{\n",
      "\"movies\": [\n",
      "{\n",
      "\"title\": \"The Shawshank Redemption (2014)\",\n",
      "\"releaseYear\": 2014,\n",
      "\"genres\": [\"Drama\", \"Crime\"],\n",
      "\"imdbRating\": 9.2,\n",
      "\"description\": \"Two men from different walks of life become unlikely friends in prison and find a way to escape.\"\n",
      "},\n",
      "{\n",
      "\"title\": \"The Grand Budapest Hotel (2014)\",\n",
      "\"releaseYear\": 2014,\n",
      "\"genres\": [\"Comedy\", \"Drama\"],\n",
      "\"imdbRating\": 8.1,\n",
      "\"description\": \"The adventures of Gustave H, a legendary concierge at the famous Grand Budapest Hotel, and Zero Moustafa, the lobby boy who becomes his most trusted friend.\"\n",
      "},\n",
      "{\n",
      "\"title\": \"Parasite (2019)\",\n",
      "\"releaseYear\": 2019,\n",
      "\"genres\": [\"Comedy\", \"Drama\"],\n",
      "\"imdbRating\": 8.7,\n",
      "\"description\": \"The Kims, a poor family of four, scheme their way into the lives of a wealthy family, the Parks.\"\n",
      "}\n",
      "]\n",
      "}\n",
      "\n",
      "These movies are highly rated and fall within the year range you specified (2010-2020). I've ordered them by descending IMDb rating to ensure that the highest-rated movie is at the top of the list. The genres listed are based on the categories provided by IMDB, but please note that some movies may defy easy categorization and could fit into multiple genres.\n",
      "\n",
      "I hope you find these recommendations helpful! Let me know if you have any other questions or preferences to refine your search further.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Imagine you are developing a movie recommendation system. Your task is to provide a list of recommended movies based\n",
    "on user preferences. The movies are from 2010 to 2020. Please only recomment movies released with this year range. Recommend only top 3 movies\n",
    "The output should be in the form of a JSON object containing the following information for each recommended movie.:\n",
    "\n",
    "1. Movie title (as a string)\n",
    "2. Release year (as an integer)\n",
    "3. Genre(s) (as an array of strings)\n",
    "4. IMDb rating (as a float with two decimal places)\n",
    "5. Description (as a string)\n",
    "\n",
    "Order the movies by descending IMDb rating.\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb9b87",
   "metadata": {
    "id": "c6cb9b87"
   },
   "source": [
    "### 4. Teaching AI how to behave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530779f",
   "metadata": {
    "id": "4530779f"
   },
   "source": [
    "- Conditional Prompting + Few-shot prompting + Step-wise Expectations\n",
    "\n",
    "**Industry Setting: Hospitality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b832d",
   "metadata": {
    "id": "1d3b832d"
   },
   "source": [
    "#### Baseline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e34d67d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e34d67d",
    "outputId": "ea09311a-d514-42a2-abb8-83d255559ae5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are some tips for responding to guest reviews:\n",
      "\n",
      "1. Respond promptly: Try to respond to all reviews within a few days of receiving them, especially if they are negative. This shows that you value your guests' feedback and are willing to address any issues or concerns they may have.\n",
      "2. Be genuine and sincere: Always be honest and authentic in your responses. Avoid canned responses or generic templates, as these can come across as insincere. Instead, focus on providing personalized responses that show you care about each guest's experience.\n",
      "3. Listen actively: Pay attention to what guests are saying and try to understand their perspective. If they have a complaint or issue, ask questions to clarify the situation and offer solutions where possible.\n",
      "4. Be respectful: Treat all guests with respect and dignity, even if they have left a negative review. Avoid getting defensive or argumentative, as this can escalate the situation and make things worse. Instead, focus on finding a resolution that works for both parties.\n",
      "5. Offer solutions: If a guest has had a negative experience, offer a solution to make it right. This could be anything from offering a refund or discount to providing additional services or amenities. Showing guests that you are willing to go the extra mile can help turn a negative review into a positive one.\n",
      "6. Follow up: If you have offered a solution to a guest, follow up to make sure it was effective and that they were satisfied with the outcome. This shows that you value their feedback and care about their experience.\n",
      "7. Use reviews as an opportunity for improvement: Negative reviews can be a valuable source of feedback for improving your business. Take the time to analyze each review and identify areas where you can improve, such as cleanliness, customer service, or amenities. Use this information to make changes that will enhance the guest experience and increase positive reviews in the future.\n",
      "\n",
      "Overall, responding to guest reviews is an important part of managing your online reputation and providing excellent customer service. By following these tips, you can show guests that you value their feedback and are committed to providing a high-quality experience for all visitors.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"How should I respond to guest reviews?\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfb462",
   "metadata": {
    "id": "cebfb462"
   },
   "source": [
    "#### Improved Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "900f1e7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "900f1e7f",
    "outputId": "47539dbb-9e66-4e1a-b174-c073cee27a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's an example of how you could craft a response to guest reviews based on their content:\n",
      "\n",
      "Positive Review:\n",
      "\n",
      "\"Wow, thank you so much for taking the time to leave such a kind review! We are thrilled to hear that you had a great experience at our hotel and we appreciate your feedback. Your satisfaction is our top priority and we're glad to see that we met or exceeded your expectations. We hope to have the pleasure of hosting you again soon! 😊\"\n",
      "\n",
      "Negative Review:\n",
      "\n",
      "\"Sorry to hear that you had a negative experience at our hotel. We apologize for any inconvenience or disappointment caused and would like to make things right. Please know that we take all feedback seriously and are committed to providing the best possible service to our guests. Can you please reach out to us directly so we can address your concerns and provide a resolution? We value your input and look forward to hearing from you soon. 😞\"\n",
      "\n",
      "Remember, when responding to negative reviews, it's important to be genuine, empathetic, and solution-oriented. Avoid getting defensive or dismissive, as this can escalate the situation and damage your reputation further. Instead, focus on providing a resolution and showing that you value the guest's feedback and business.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Craft a response to guest reviews: If the review is positive, thank the guest for their feedback and encourage them to return. If the review is negative, apologize for their experience and offer to resolve the issue directly.\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db441a5",
   "metadata": {
    "id": "2db441a5"
   },
   "source": [
    "#### Conditional Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d6e2b0ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6e2b0ed",
    "outputId": "62094ce3-d060-41de-d6d7-f8a276a9b3db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! Here is the customer review with the sentiment classified as \"angry\":\n",
      "\n",
      "Customer Review:\n",
      "I am extremely disappointed with the service I received at your store! The staff was rude and unhelpful, showing no regard for my concerns. Not only did they ignore my requests for assistance, but they also had the audacity to speak to me condescendingly. It's clear that your company values profit over customer satisfaction. I will never shop here again and will make sure to spread the word about my awful experience. You've lost a loyal customer, and I hope others steer clear of your establishment!\n",
      "\n",
      "Sentiment: Angry\n",
      "\n",
      "Here is the apology response:\n",
      "\n",
      "Dear [Customer Name],\n",
      "\n",
      "We are truly sorry for the poor service you received at our store. We understand that our staff's behavior was unacceptable, and we take full responsibility for their actions. We value your business and would like to make things right. Please reach out to us directly so we can address your concerns and provide a resolution. Your satisfaction is of the utmost importance to us, and we hope you will give us another chance to serve you better in the future.\n",
      "\n",
      "Thank you for bringing this to our attention.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"Here is the customer review {customer_review}\n",
    "\n",
    "Check the sentiment of the customer and classify it as “angry” or “happy”\n",
    "If the customer is “angry” - reply starting with an apology\n",
    "Else - just thank the customer\n",
    "\n",
    "customer_review = \"\n",
    "I am extremely disappointed with the service I received at your store! The staff was rude and unhelpful, showing no regard for my concerns. Not only did they ignore my requests for assistance, but they also had the audacity to speak to me condescendingly. It's clear that your company values profit over customer satisfaction. I will never shop here again and will make sure to spread the word about my awful experience. You've lost a loyal customer, and I hope others steer clear of your establishment!\n",
    "\"\n",
    "\n",
    "\n",
    "Here is the customer review {customer_review}\n",
    "\n",
    "Check the sentiment of the customer and classify it as “angry” or “happy”\n",
    "If the customer is “angry” - reply starting with an apology\n",
    "Else - just thank the customer\n",
    "\n",
    "customer_review = \"\n",
    "I couldn't be happier with my experience at your store! The staff went above and beyond to assist me, providing exceptional customer service. They were friendly, knowledgeable, and genuinely eager to help. The product I purchased exceeded my expectations and was exactly what I was looking for. From start to finish, everything was seamless and enjoyable. I will definitely be returning and recommending your store to all my friends and family. Thank you for making my shopping experience so wonderful!\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b0328",
   "metadata": {
    "id": "ef8b0328"
   },
   "source": [
    "#### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "250e9109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "250e9109",
    "outputId": "9c98856d-bfa6-4db0-c119-8df1bf0577ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are three distinct animals, highlighting their unique characteristics and habitats:\n",
      "\n",
      "Animal: Lion\n",
      "Description: The lion is a majestic big cat known for its regal mane and powerful roar. It is one of the largest predatory cats in Africa and can be found in savannahs and grasslands. Lions are social animals that live in prides, which are typically made up of several females, their cubs, and one or more males. They are skilled hunters and use coordinated attacks to bring down their prey.\n",
      "\n",
      "Animal: Duck\n",
      "Description: Ducks are aquatic birds known for their distinct webbed feet and flat bills. There are over 120 species of ducks found in wetlands around the world, from ponds and lakes to rivers and oceans. Many duck species migrate seasonally, while others remain year-round in their habitats. Ducks are social birds that often form large flocks and communicate with each other through quacks and body language.\n",
      "\n",
      "Animal: Monkey\n",
      "Description: Monkeys are primates known for their agility and intelligence. There are over 260 species of monkeys found in tropical forests around the world, from the tiny pygmy marmoset to the large mandrill. Many monkey species have prehensile tails that they use as an extra hand to swing through the trees. Monkeys are social animals that live in troops and communicate with each other through vocalizations and body language. They are known for their playful behavior and curious nature, often exploring their surroundings with a mischievous twinkle in their eyes.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Teacher prompt: There are countless fascinating animals on Earth. In just a few shots, describe three distinct animals, highlighting their unique characteristics and habitats.\n",
    "\n",
    "Student response:\n",
    "\n",
    "Animal: Tiger\n",
    "Description: The tiger is a majestic big cat known for its striking orange coat with black stripes. It is one of the largest predatory cats in the world and can be found in various habitats across Asia, including dense forests and grasslands. Tigers are solitary animals and highly territorial. They are known for their exceptional hunting skills and powerful builds, making them apex predators in their ecosystems.\n",
    "\n",
    "Animal: Penguin\n",
    "Description: Penguins are flightless birds that have adapted to life in the Southern Hemisphere, particularly in Antarctica. They have a distinct black and white plumage that helps camouflage them in the water, while their streamlined bodies enable swift swimming. Penguins are well-suited for both land and sea, and they often form large colonies for breeding and raising their young. These social birds have a unique waddling walk and are known for their playful behavior.\n",
    "\n",
    "Animal: Elephant\n",
    "Description: Elephants are the largest land mammals on Earth. They have a characteristic long trunk, which they use for various tasks such as feeding, drinking, and social interaction. Elephants are highly intelligent and display complex social structures. They inhabit diverse habitats like savannahs, forests, and grasslands in Africa and Asia. These gentle giants have a deep connection to their families and are known for their exceptional memory and empathy.\n",
    "\n",
    "Do this for Lion, Duck, and Monkey\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216808bb",
   "metadata": {
    "id": "216808bb"
   },
   "source": [
    "#### Marketing Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "138b7194",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "138b7194",
    "outputId": "b08678f7-bd21-41b1-f7ac-80d61e4a5d8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are two distinct marketing strategies for a product launch campaign, highlighting their key points, pros, cons, and risks:\n",
      "\n",
      "1. **Public Relations (PR):**\n",
      "   - Key Points: Utilizes media coverage to build brand awareness, credibility, and reputation.\n",
      "   - Pros: High perceived value, third-party endorsement, ability to reach a wider audience.\n",
      "   - Cons: Limited control over messaging, potential for negative publicity if not managed properly.\n",
      "   - Risks: Dependence on media coverage, potential for crisis or reputation damage if not handled effectively.\n",
      "\n",
      "2. **Product Collaborations:**\n",
      "   - Key Points: Partners with other brands or influencers to co-create products or promote existing ones.\n",
      "   - Pros: Increased brand exposure, access to new audiences, potential for cross-promotion and bundling.\n",
      "   - Cons: Difficulty in finding the right partners, potential creative differences, risks associated with joint branding.\n",
      "   - Risks: Dependence on partner's reputation, potential for negative association if not managed properly.\n",
      "\n",
      "In both cases, it is important to carefully consider the pros and cons of each strategy and assess which approach aligns best with your product launch goals and target audience. Additionally, a thorough risk analysis should be conducted to ensure that any chosen marketing strategy does not pose significant risks to the brand or its reputation.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Below we have described two distinct marketing strategies for a product launch campaigns,\n",
    "highlighting their key points, pros, cons and risks.\n",
    "\n",
    "1. **Digital Marketing:**\n",
    "   - Key Points: Utilizes online platforms to promote the product, engage with the audience, and drive traffic to the product website.\n",
    "   - Pros: Wide reach, targeted audience segmentation, cost-effective, ability to track and measure results.\n",
    "   - Cons: High competition, rapidly evolving digital landscape, ad fatigue.\n",
    "   - Risks: Negative feedback or criticism can spread quickly online, potential for ad fraud or click fraud.\n",
    "\n",
    "2. **Traditional Advertising:**\n",
    "   - Key Points: Uses traditional media channels like TV, radio, and print to reach a broader audience.\n",
    "   - Pros: Wide reach, brand visibility, potential to reach a diverse audience.\n",
    "   - Cons: High cost, difficulty in targeting specific demographics, less trackability compared to digital channels.\n",
    "   - Risks: Limited audience engagement, potential for ad avoidance or low attention.\n",
    "\n",
    "Now as described above can you do this for do this for 1) Public Relations(PR) and 2) Product Collaborations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fcadd",
   "metadata": {
    "id": "885fcadd"
   },
   "source": [
    "#### Stepwise Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b216fca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b216fca",
    "outputId": "36bbdd84-63f5-4704-bbdd-a0fcad4d1f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are the answers to your questions:\n",
      "\n",
      "1. English translation of the article:\n",
      "\n",
      "\"Climate change remains a pressing concern in Europe, with the region experiencing an increase in extreme weather events over the past decades, such as deadly heatwaves and devastating floods. These events have highlighted the urgent need to address climate change and its impacts. Europe has committed to leading global efforts to combat climate change, with several European countries setting ambitious emissions reduction targets and implementing policies to promote renewable energy and energy efficiency. However, challenges persist, as some regions in Europe still rely heavily on fossil fuels, making the transition to a low-carbon economy difficult. International cooperation is essential, as climate change transcends national borders. The economic implications of climate action in Europe are also significant, with the transition towards a sustainable economy potentially creating job opportunities and promoting technological innovation.\"\n",
      "\n",
      "2. Summary of the article in 30 words:\n",
      "\n",
      "Europe is taking measures to combat climate change, but challenges persist due to reliance on fossil fuels and need for international cooperation.\n",
      "\n",
      "3. Tags for the summary:\n",
      "\n",
      "[ClimateChange], [Environment], [Technology], [Healthcare], [Education], [Business], [ArtificialIntelligence], [Travel], [Sports], [Fashion], [Entertainment], [Science]\n",
      "\n",
      "4. JSON file for all the tags with values 1 if present, and 0 if not:\n",
      "```json\n",
      "{\n",
      "\"ClimateChange\": 1,\n",
      "\"Environment\": 1,\n",
      "\"Technology\": 1,\n",
      "\"Healthcare\": 0,\n",
      "\"Education\": 0,\n",
      "\"Business\": 1,\n",
      "\"ArtificialIntelligence\": 0,\n",
      "\"Travel\": 0,\n",
      "\"Sports\": 0,\n",
      "\"Fashion\": 0,\n",
      "\"Entertainment\": 0,\n",
      "\"Science\": 1\n",
      "}\n",
      "```\n",
      "5. Segregate the tags based on 1 and 0:\n",
      "\n",
      "Tags with value 1 (present in the summary):\n",
      "\n",
      "* ClimateChange\n",
      "* Environment\n",
      "* Technology\n",
      "* Business\n",
      "* Science\n",
      "\n",
      "Tags with value 0 (not present in the summary):\n",
      "\n",
      "* Healthcare\n",
      "* Education\n",
      "* ArtificialIntelligence\n",
      "* Travel\n",
      "* Sports\n",
      "* Fashion\n",
      "* Entertainment\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"El cambio climático continúa siendo una preocupación apremiante en Europa.\n",
    "La región ha experimentado un aumento en eventos climáticos extremos en las últimas décadas, desde olas de calor mortales\n",
    "hasta inundaciones devastadoras. Estos eventos extremos han dejado en claro la urgente necesidad de abordar el cambio climático y sus impactos.\n",
    "Europa se ha comprometido a liderar los esfuerzos mundiales para combatir el cambio climático.\n",
    "Varios países europeos han establecido ambiciosos objetivos de reducción de emisiones y han implementado políticas para promover la energía\n",
    "renovable y la eficiencia energética. La Unión Europea ha adoptado el Acuerdo Verde Europeo, un plan integral para lograr la neutralidad de\n",
    "carbono para 2050.Sin embargo, los desafíos persisten. Algunas regiones de Europa aún dependen en gran medida de combustibles fósiles,\n",
    "lo que dificulta la transición hacia una economía baja en carbono. Además, la cooperación internacional es fundamental, ya que el\n",
    "cambio climático trasciende las fronteras nacionales.La acción climática en Europa también tiene implicaciones económicas.\n",
    "La transición hacia una economía sostenible puede generar oportunidades de empleo y promover la innovación tecnológica.En resumen, Europa reconoce la gravedad del cambio climático y está tomando medidas significativas para abordar esta crisis. Sin embargo, se necesita un esfuerzo colectivo continuo y una cooperación global para enfrentar los desafíos planteados por el cambio climático y garantizar un futuro sostenible para Europa y el resto del mundo.”\n",
    "\n",
    "1. Change the above article from Spanish to English\n",
    "2. Summarize this article in 30 words\n",
    "3. Check the tags for the summary from the tags list (ClimateChange, Environment, Technology, Healthcare, Education, Business, ArtificialIntelligence, Travel, Sports, Fashion, Entertainment, Science)\n",
    "4. Create a JSON file for all the tags with values 1 if the tag is present, and 0 if not in the above summary\n",
    "5. Segregate the tags based on 1 and 0\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054ac70",
   "metadata": {
    "id": "c054ac70"
   },
   "source": [
    "### 5. Teaching AI how to think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc58e86",
   "metadata": {
    "id": "5bc58e86"
   },
   "source": [
    "- Asking the model to analyze, relate, and ask you questions before it replies/reaches a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d53170",
   "metadata": {
    "id": "53d53170"
   },
   "source": [
    "#### Make it ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f33d7855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f33d7855",
    "outputId": "040ce182-063e-47b7-c910-ed70a798130a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! Before I suggest a gaming laptop, can you please tell me a bit more about your needs and preferences? Here are some questions that will help me narrow down the options:\n",
      "\n",
      "1. What is your budget for this laptop?\n",
      "2. Do you have any specific game titles or genres that you're interested in playing on this laptop?\n",
      "3. How important is portability to you - do you need a laptop that's lightweight and easy to carry around, or are you more focused on power and performance?\n",
      "4. Are there any other features that you consider must-haves for your gaming laptop (e.g. touchscreen display, long battery life, etc.)?\n",
      "5. Do you have a preferred operating system - Windows, macOS, or Linux?\n",
      "\n",
      "Once I have a better understanding of your needs and preferences, I can suggest some great options for gaming laptops that fit your criteria!\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Suggest one Gaming Laptop. Ask me relevant questions before you choose\"\"\"\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd33a8",
   "metadata": {
    "id": "a1fd33a8"
   },
   "source": [
    "#### Teach it how to engineer something before asking it to generate the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9969a335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9969a335",
    "outputId": "aa9848bb-096d-4849-ec32-263a84f4ea4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As an engineer tasked with designing a renewable energy system for a remote island community that currently relies on diesel generators for electricity, I have conducted a comprehensive analysis of the island's energy demand patterns, resource availability, and technical, economic, environmental, and social factors. Based on my findings, I recommend a hybrid renewable energy system consisting of solar photovoltaic (PV), wind turbines, and battery energy storage to meet the island's power demands sustainably and reliably.\n",
      "\n",
      "Energy Demand Analysis:\n",
      "\n",
      "a. Determine the island's energy consumption patterns and peak demand:\n",
      "The remote island community has a population of approximately 2,000 residents and consumes an average of 3,500 MWh of electricity per year. The peak demand occurs during summer months when air conditioning and water pumping are in high demand.\n",
      "\n",
      "b. Analyze any anticipated future growth in energy demand:\n",
      "The island's population is expected to grow by 10% over the next five years, which will increase the annual electricity consumption by approximately 350 MWh.\n",
      "\n",
      "Resource Assessment:\n",
      "\n",
      "a. Evaluate the island's geographical location and climate conditions to identify available renewable energy resources:\n",
      "The remote island is located in a tropical region with high levels of solar irradiance and wind speeds, making it an ideal location for solar PV and wind turbines.\n",
      "\n",
      "b. Assess the variability and intermittency of these resources to determine their reliability and potential for power generation:\n",
      "Solar PV and wind energy are both subject to seasonal variations and weather patterns, but they can be combined with battery storage to ensure a reliable supply of electricity.\n",
      "\n",
      "System Design and Integration:\n",
      "\n",
      "a. Propose an optimal mix of renewable energy technologies based on the resource assessment and energy demand analysis:\n",
      "A hybrid system consisting of solar PV (60%), wind turbines (30%), and battery energy storage (10%) will meet the island's power demands sustainably and reliably. The system will be designed to provide a minimum of 95% reliability, ensuring that the community has access to electricity at all times.\n",
      "\n",
      "b. Address any technical challenges, such as grid integration, energy storage, and voltage regulation:\n",
      "The hybrid system will be integrated into the existing diesel-based grid, with a smart controller managing the flow of power between the renewable sources, battery storage, and the grid. The system will also include energy management software to optimize energy usage and minimize waste.\n",
      "\n",
      "Economic Viability:\n",
      "\n",
      "a. Perform a cost analysis comparing the renewable energy system with the existing diesel generator setup:\n",
      "The total cost of ownership (TCO) for the hybrid renewable energy system over 20 years will be approximately $15 million, while the TCO for the existing diesel generators over the same period is estimated to be around $20 million. The renewable energy system will provide significant savings in fuel costs and maintenance expenses.\n",
      "\n",
      "b. Consider the initial investment, operational costs, maintenance requirements, and potential government incentives or subsidies:\n",
      "The island's government offers a 30% tax credit for renewable energy projects, which can be used to offset some of the upfront costs. Additionally, the hybrid system will have lower operational costs compared to diesel generators due to reduced fuel consumption and maintenance requirements.\n",
      "\n",
      "Environmental Impact:\n",
      "\n",
      "a. Assess the environmental benefits of transitioning to renewable energy, such as reduced greenhouse gas emissions and local pollution:\n",
      "The hybrid renewable energy system will reduce the island's carbon footprint by approximately 70%, minimizing the impact on local ecosystems and wildlife.\n",
      "\n",
      "b. Consider the potential impact on local ecosystems and wildlife, ensuring that the chosen technologies minimize negative effects:\n",
      "The solar PV panels will be installed on existing rooftops or in a ground-mounted array to minimize land use and habitat disruption. The wind turbines will be located in an area away from residential areas and wildlife habitats, with noise reduction measures implemented to minimize impacts on local fauna.\n",
      "\n",
      "Implementation and Operations:\n",
      "\n",
      "a. Develop an implementation plan, including the timeline, procurement of equipment, and construction considerations:\n",
      "The hybrid renewable energy system will be installed over a period of 12 months, with solar PV panels and wind turbines being erected simultaneously to minimize delays. The existing diesel\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"You are an engineer tasked with designing a renewable energy system for a remote island community that currently relies on diesel generators for electricity. The island has limited access to fuel and experiences frequent power outages due to logistical challenges and adverse weather conditions. Your goal is to develop a sustainable and reliable energy solution that can meet the island's power demands. Consider the following factors in your analysis and provide your recommendations:\n",
    "\n",
    "Energy Demand Analysis:\n",
    "a. Determine the island's energy consumption patterns and peak demand.\n",
    "b. Analyze any anticipated future growth in energy demand.\n",
    "\n",
    "Resource Assessment:\n",
    "a. Evaluate the island's geographical location and climate conditions to identify available renewable energy resources (e.g., solar, wind, hydro, geothermal).\n",
    "b. Assess the variability and intermittency of these resources to determine their reliability and potential for power generation.\n",
    "\n",
    "System Design and Integration:\n",
    "a. Propose an optimal mix of renewable energy technologies based on the resource assessment and energy demand analysis.\n",
    "b. Address any technical challenges, such as grid integration, energy storage, and voltage regulation.\n",
    "\n",
    "Economic Viability:\n",
    "a. Perform a cost analysis comparing the renewable energy system with the existing diesel generator setup.\n",
    "b. Consider the initial investment, operational costs, maintenance requirements, and potential government incentives or subsidies.\n",
    "\n",
    "Environmental Impact:\n",
    "a. Assess the environmental benefits of transitioning to renewable energy, such as reduced greenhouse gas emissions and local pollution.\n",
    "b. Consider the potential impact on local ecosystems and wildlife, ensuring that the chosen technologies minimize negative effects.\n",
    "\n",
    "Implementation and Operations:\n",
    "a. Develop an implementation plan, including the timeline, procurement of equipment, and construction considerations.\n",
    "b. Outline an operational strategy, including maintenance schedules, training requirements, and emergency response protocols.\n",
    "\n",
    "Based on your analysis, provide a well-reasoned recommendation for the most suitable renewable energy system for the remote island, considering factors such as reliability, scalability, economic viability, and environmental sustainability.\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a3351",
   "metadata": {
    "id": "f54a3351"
   },
   "source": [
    "### 6. Extracting and Filtering Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d656472",
   "metadata": {
    "id": "6d656472"
   },
   "source": [
    "**Industry Setting: Market Research**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f7aff",
   "metadata": {
    "id": "135f7aff"
   },
   "source": [
    "#### Baseline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee5fda35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee5fda35",
    "outputId": "d7767b47-4f62-4c2f-fa7d-bc737e07d817"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I can help you extract the information from the product reviews! Here are the answers to your questions based on each review:\n",
      "\n",
      "1. Phone model: Review-1: XUI890; Review-2: ZetaPhone Z5; Review-3: TechPro X8; Review-4: UNKNOWN (no mention of phone model)\n",
      "2. Price: Review-1: 1500 $; Review-2: 1200 $; Review-3: 900 $; Review-4: 1400 $\n",
      "3. Complaint description: Review-1: \"a colossal disappointment... money pit\"; Review-2: \"complete rip-off\"; Review-3: \"utter waste of money\"; Review-4: \"an outright scam\"\n",
      "4. Additional charges: None mentioned in any review\n",
      "5. Refund expected: Review-1: TRUE; Review-2: TRUE; Review-3: UNKNOWN (no mention of refund); Review-4: TRUE\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Extract the below information from the product reviews:\n",
    "\n",
    "1. phone_model: This is the name of the phone - if unknown, just say “UNKNOWN”.\n",
    "2. phone_price: The price in dollars - if unknown, assume it to be 1000 $.\n",
    "3. complaint_desc: A short description/summary of the complaint in less than 20 words.\n",
    "4. additional_charges: How much in dollars did the customer spend to fix the problem? - this should be an integer.\n",
    "5. refund_expected: TRUE or FALSE - check if the customer explicitly mentioned the word “refund” to tag as TRUE. If unknown, assume that the customer is not expecting a refund.\n",
    "\n",
    "Here are the reviews:\n",
    "- Review-1: “I am fuming with anger and regret over my purchase of the XUI890. First, the price tag itself was exorbitant at 1500 $, making me expect exceptional quality. Instead, it turned out to be a colossal disappointment... money pit. Beware, fellow buyers!”\n",
    "- Review-2: “I am beyond furious with my purchase of the ZetaPhone Z5! The $1200 price tag should have guaranteed excellence, but it was a complete rip-off... avoid the ZetaPhone Z5 at all costs!”\n",
    "- Review-3: “Purchasing the TechPro X8 for $900 was the biggest mistake of my life. I expected a top-notch device, but it was a complete disaster... utter waste of money.”\n",
    "- Review-4: “This phone left me seething with anger and regret. Spending $1400 on this phone was an outright scam... pitiful excuse of a phone.”\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2237a59",
   "metadata": {
    "id": "c2237a59"
   },
   "source": [
    "#### Improved Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41ce187d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41ce187d",
    "outputId": "f4658503-f776-48b3-adf7-47fee884ce5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is the JSON output based on the provided product reviews for phones sold on Amazon, with the specified headers:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"phone_model\": \"XUI890\",\n",
      "        \"phone_price\": 1500,\n",
      "        \"complaint_desc\": \"colossal disappointment, money pit\",\n",
      "        \"additional_charges\": null,\n",
      "        \"refund_expected\": true\n",
      "    },\n",
      "    {\n",
      "        \"phone_model\": \"ZetaPhone Z5\",\n",
      "        \"phone_price\": 1200,\n",
      "        \"complaint_desc\": \"complete rip-off\",\n",
      "        \"additional_charges\": null,\n",
      "        \"refund_expected\": true\n",
      "    },\n",
      "    {\n",
      "        \"phone_model\": \"TechPro X8\",\n",
      "        \"phone_price\": 900,\n",
      "        \"complaint_desc\": \"utter waste of money\",\n",
      "        \"additional_charges\": null,\n",
      "        \"refund_expected\": true\n",
      "    },\n",
      "    {\n",
      "        \"phone_model\": \"UNKNOWN\",\n",
      "        \"phone_price\": 1400,\n",
      "        \"complaint_desc\": \"pitiful excuse of a phone\",\n",
      "        \"additional_charges\": null,\n",
      "        \"refund_expected\": true\n",
      "    }\n",
      "]\n",
      "\n",
      "Note that for the last review, the phone model is unknown, so it is listed as \"UNKNOWN\" in the output. Additionally, the price of $1400 is used as a default value for additional charges, as no specific amount was mentioned in the review.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\"Please extract the specified information from the following product reviews for phones sold on Amazon. Ensure clarity and accuracy in your JSON output based on the provided headers. The reviews contain customer experiences, and you should focus on summarizing their main complaints effectively.\n",
    "\n",
    "**JSON Headers:**\n",
    "1. **phone_model:** Name of the phone mentioned in the review. If not specified, use “UNKNOWN”.\n",
    "2. **phone_price:** Price of the phone in dollars. If the price is unclear, default to 1000 $.\n",
    "3. **complaint_desc:** A concise summary of the complaint, limited to 20 words or fewer.\n",
    "4. **additional_charges:** Total additional costs incurred by the customer for repairs, represented as an integer.\n",
    "5. **refund_expected:** Set to TRUE if the customer explicitly requests a “refund”; otherwise, set to FALSE.\n",
    "\n",
    "**Product Reviews:**\n",
    "- Review-1: “I am fuming with anger and regret over my purchase of the XUI890. First, the price tag itself was exorbitant at 1500 $, making me expect exceptional quality. Instead, it turned out to be a colossal disappointment... money pit. Beware, fellow buyers!”\n",
    "- Review-2: “I am beyond furious with my purchase of the ZetaPhone Z5! The $1200 price tag should have guaranteed excellence, but it was a complete rip-off... avoid the ZetaPhone Z5 at all costs!”\n",
    "- Review-3: “Purchasing the TechPro X8 for $900 was the biggest mistake of my life. I expected a top-notch device, but it was a complete disaster... utter waste of money.”\n",
    "- Review-4: “This phone left me seething with anger and regret. Spending $1400 on this phone was an outright scam... pitiful excuse of a phone.”\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc312d60",
   "metadata": {
    "id": "dc312d60"
   },
   "source": [
    "### 7. Other Use-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1f38e",
   "metadata": {
    "id": "1af1f38e"
   },
   "source": [
    "#### Grammar and Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e392b66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e392b66",
    "outputId": "e5d73399-0e1f-4711-c951-f33506f3719c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Here's the proofread version of the email:\n",
      "\n",
      "Dear Sir/Madam,\n",
      "\n",
      "I am writing to inquire about the availability of your product. I saw it on your website and it looks very interesting. Could you please send me more information regarding pricing and shipping options? Additionally, do you have any discounts available for bulk orders? I would appreciate it if you could get back to me as soon as possible. My company is interested in purchasing your product for our upcoming project.\n",
      "\n",
      "Thank you in advance for your assistance.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Here are some corrections and suggestions:\n",
      "\n",
      "1. \"avaliability\" should be spelled \"availability\".\n",
      "2. \"pricig\" should be spelled \"pricing\".\n",
      "3. \"shippng\" should be spelled \"shipping\".\n",
      "4. \"optins\" should be spelled \"options\".\n",
      "5. \"bulck\" should be spelled \"bulk\".\n",
      "6. Add a comma after \"project\" in the last sentence for better clarity.\n",
      "7. Use a colon instead of a comma to separate the items in the list (e.g., \"pricing, shipping options\").\n",
      "8. Consider adding your company name or contact information at the end of the email so that the recipient can easily get back to you with any questions or concerns.\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"Dear Sir/Madam,\n",
    "I am writting to inqure about the avaliability of your produc. I saw it on your websit and it looks very intresting. Can you plase send me more informtion regaring pricig and shippng optins? Also, do you have any discounts avilable for bulck orders? I would appriciate if you could get back to me as soon as possble. My company is intersted in purchsing your produc for our upcomimg projct. Thank you in advanc for your assistnce.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "\n",
    "Can you proofread the above text ?\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9dd34",
   "metadata": {
    "id": "cdb9dd34"
   },
   "source": [
    "#### Changing the tone of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85fe9cad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85fe9cad",
    "outputId": "8f94198d-27e4-4f4a-bf3b-4a476a61f800"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are the three conversions:\n",
      "\n",
      "Neutral tone:\n",
      "\"I am disappointed with my recent purchase of a phone. While I was initially excited about the device, it has been plagued by software glitches and constant crashes since day one. Additionally, the charging port became faulty within two weeks, requiring an extra $100 for repairs. The camera also stopped functioning properly, and the repair quote was surprisingly high at $500. Despite these issues, I am still hoping for a resolution to make this experience more satisfactory.\"\n",
      "\n",
      "Humorous tone:\n",
      "\"Oh boy, where do I begin? My new phone has been a real treat! It's like a never-ending game of \"Angry Birds\" with glitches and crashes everywhere. The charging port decided to take a vacation after just two weeks, leaving me feeling drained (get it?) and the camera is now more useful as a paperweight. But hey, at least I'm getting a great workout in from all the throwing my phone against the wall! Can someone please pass the Advil? Oh, and by the way, can I get a refund or something?\"\n",
      "\n",
      "Angrier tone:\n",
      "\"ARE YOU KIDDING ME RIGHT NOW?! This so-called \"phone\" is nothing but a piece of junk! From day one, it's been plagued by glitches and crashes that make it virtually unusable. The charging port broke within two weeks, costing me an extra $100 for repairs. And to top it all off, the camera stopped working properly and the repair quote is a whopping $500! I demand an apology and a full refund for this piece of garbage. How dare you sell such a defective product to unsuspecting customers?!\"\n"
     ]
    }
   ],
   "source": [
    "user_prompt =\"\"\"This phone left me seething with anger and regret. Spending $1400 on this phone was an outright scam. The device was riddled with issues from day one. The software glitches made it virtually unusable, and the constant crashes were infuriating. To add insult to injury, the charging port became faulty within two weeks, costing me an extra $100 for repairs. And guess what? The camera stopped functioning properly, and the repair quote was a shocking $500! I demand an apology for this pitiful excuse of a phone.\n",
    "\n",
    "Convert this angry review into a neutral tone\n",
    "Convert this angry review into a humorous tone\n",
    "Convert this angry review into an angrier tone\"\"\"\n",
    "\n",
    "response = generate_llama_response(user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4e92c",
   "metadata": {
    "id": "18f4e92c"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2818b2f",
   "metadata": {
    "id": "d2818b2f"
   },
   "source": [
    "**Evaluating Summarization Capability**\n",
    "\n",
    "There are two methods to summarize input text:\n",
    "- Abstractive (output: a gist of the input).\n",
    "- Extractive (output: a selection of key sentences from the input).\n",
    "\n",
    "The objective in abstractive summarization is to generate a clear summary of the input text, while that of extractive summarization is to generate a selection of appropriate sentences that summarize the input text.\n",
    "\n",
    "In order to evaluate model predictions, we compare the model predictions with the ground truth on a sample of human-annotated gold examples. However, given the subjective nature of model predictions, we need new metrics to evaluate summarization outputs: ROUGE Score and BERT Score.\n",
    "\n",
    "Apart from these automated metrics, another method used to judge the quality of a summary is to use another LLM to assign a quality rating to the summary. Using an LLM to avaluate another LLM offers further flexibility in evaluation. For example, we could also specify specific attributes of an ideal summary (for e.g., conciseness, clarity of exposition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7628b",
   "metadata": {
    "id": "d0e7628b"
   },
   "source": [
    "### Recall-Oriented Understudy for Gisting Evaluation (ROUGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e5a0e",
   "metadata": {
    "id": "d42e5a0e"
   },
   "source": [
    "ROUGE takes a exact match approach to compare the prediction from the model with the human reference summary by relying on matches of n-grams between the two.\n",
    "\n",
    "The [$\\text{ROUGE}_{N}$](https://huggingface.co/spaces/evaluate-metric/rouge) score is computed using the ratio of the number of n-gram matches to the total number of n-grams in the human generated summary. However, we still have to make a choice whether unigram, bigram, or any other n-gram should be used.\n",
    "\n",
    "To solve this conundrum, a common variant of ROUGE that is used to generate a comparison metric is $\\text{ROUGE}_{\\text{L}}$, where we first compute the recall and precision of the longest common subsequence and then compute the harmonic mean of these values (punctuation and case of the word are disregarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c722f24",
   "metadata": {
    "id": "3c722f24"
   },
   "outputs": [],
   "source": [
    "ai_generated_summary = \"Alice and Ben boarded a train to Mexico.\"\n",
    "human_generated_summary = \"Alice and Ben boarded their train to Mexico for vacation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d59396",
   "metadata": {
    "id": "c7d59396"
   },
   "source": [
    "The length of the largest common subsequence (LCS) between the two summaries is 5. The number of unigrams in the AI-generated summary is 10 and the number of unigrams in the human generated summary is 17.\n",
    "\n",
    "We define the recall of the LCS as 5/17 and the precision of the LCS as 5/10 (notice the parallel with the precision and recall measures used to evaluate classification tasks).\n",
    "\n",
    "From these measures, we can compute $\\text{ROUGE}_{\\text{L}}$ as the F1 score associated with the precision and recall like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f85072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1f85072",
    "outputId": "9f4fbe39-2f94-4059-f89b-44d064821b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L: 0.7777777777777777\n"
     ]
    }
   ],
   "source": [
    "# ROUGE-L\n",
    "r_lcs, p_lcs = 7/10, 7/8\n",
    "print('ROUGE-L:', (2 * r_lcs * p_lcs)/(r_lcs + p_lcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db65df",
   "metadata": {
    "id": "40db65df"
   },
   "source": [
    "One important limitation of ROUGE is that it accounts for exact matches. This means that a summary that uses semantically close words would receive a poor score despite capturing the intent of the human summary. Hence, ROUGE is usually used for extractive summarization.\n",
    "\n",
    "ROUGE values close to 1 indicate that the AI-generated text is close to the text generated by a human."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b6e9c",
   "metadata": {
    "id": "905b6e9c"
   },
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7209cfd",
   "metadata": {
    "id": "f7209cfd"
   },
   "source": [
    "BERTScore is ideal in situations where abstractive summarization is the objective (as is in this case). To illustrate the computation of the BERTScore, consider the following two summary outputs (one from a generative AI model and another from a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f8a541",
   "metadata": {
    "id": "76f8a541"
   },
   "outputs": [],
   "source": [
    "ai_generated_summary = \"Major issues, malfunctioning camera.\"\n",
    "human_generated_summary = \"Severely disappointed, constant problems.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bc012",
   "metadata": {
    "id": "703bc012"
   },
   "source": [
    "- Look at the two summaries presented above. Though the choice of words is not exactly the same, both are close in intent.\n",
    "- In order to capture intent, we use specific models that encode the semantic meaning of words used in the models in a mathematical space where we can measure the distance between the words used.\n",
    "- Since distances can be computed, if two words are close to each other in this mathematical space (i.e., less distance), we can infer that these two words are close in meaning.\n",
    "- Models that encode this mapping, that is, models that associate words with a list of numbers (called *vectors*) that define positions of the words in a mathematical space are referred to as [*embedding models*](https://projector.tensorflow.org/).\n",
    "- Embedding models are precursors to language models and are a crucial component of how we represent the semantic meaning of words used in text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49a36c",
   "metadata": {
    "id": "df49a36c"
   },
   "source": [
    "BERTScore uses one such pre-trained embedding model (i.e., Bi-directional Encoder Representation from Transformers - BERT) to:\n",
    "- map individual words in sentences (in both the AI summary and the human summary) to vectors.\n",
    "- compute pairwise similarity between all possible pairs of words using these vectors.\n",
    "\n",
    "Once pairwise similarities are estimated, we use these similarities to compute precision and recall for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fc927",
   "metadata": {
    "id": "414fc927"
   },
   "source": [
    "- The candidate text (i.e., the AI-generated summary) and the reference text (i.e., the human generated summary) are tokenized and assigned a numeric representation.\n",
    "- In this representation, tokens that are correlated with each other lie close to each other (in the space defined by the numeric representation).\n",
    "- All pairwise correlations between the tokens of the candidate and the reference are then collected in a table.\n",
    "- For precision ($P$), we average the maximum correlation scores for each token in the *candidate*.\n",
    "- For recall ($R$), we average the maximum correlation scores for each token in the *reference*.\n",
    "- The F1 score is estimated as: $(2 \\times P \\times R)/(P+ R)$.\n",
    "- We report the F1 score as the BERTScore. As with ROUGE, BERTScores close to 1 are considered ideal (i.e., the AI-generated text is close to one that is produced by a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8sxj0mW7hKas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8sxj0mW7hKas",
    "outputId": "851dcee9-92a3-4cf0-c293-5aa437960854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.52.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "dd51366c62f54ba8978002a99d17c979",
       "pip_warning": {
        "packages": [
         "nvidia"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install evaluate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7418b00d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7418b00d",
    "outputId": "68cf25a4-ad9a-4b53-f23b-b6f6675974ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bert_scorer = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e4ebff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "6beecc1d94654e7dbc7246b5880b3972",
      "ec07268e2e844de4bf99de4dcc878fb8",
      "b4ea514293a841189864c17047203aa4",
      "1c283a4632054e2ea5a050fc6b90cb9f",
      "a084526c616f4d8e988947e06f7a66a4",
      "be5f09747bdc4650907fbb6ca53fd3f8",
      "546fc2f8fbc14fd895a2382527f59aba",
      "4d49cb38e3a144d683596b890e039757",
      "6025fcb6ef6e41638d812d5da6147e8f",
      "f1902cf27cd1422e8990e4209aee793f",
      "d1d4388469ac4f37bd968c25181a904a",
      "29f342b86fe040bdb359901814c4000d",
      "e122a75597fa4fc1be716907ea099eb4",
      "4f1cbef754794acfb371f40ef114924e",
      "b72dd63481504799bcc48d6d36da9121",
      "f207525aff5c4c8298db1f3945929c26",
      "22ea93f636c34339aa30620d17437ea0",
      "ca86e127b7a74d32aae9daac95ea4432",
      "3e30273fd6744f98b21fc4a57971d96a",
      "d71f049a3d164e1780a01bf7b529a298",
      "3c1f914466954a8f9d4cdde7d27262bc",
      "5e14a7a91ef74fb38536aeb9f70cc61a",
      "18b77444da2c4bdd8b162bf5af0b43bf",
      "2e15c87ed82c4a4a85c1263e328aba89",
      "c9141543d5cb4ffdb89967021251cc99",
      "74724ec061594cae874842a1560a8a31",
      "967b552d6b2941fe9fccd63e84800787",
      "4a2892d85c50424bbf800909013d2e7e",
      "c26aebd65d1b49bf8e7c433b3fd7022f",
      "d4d74514de0e41f4a7d3fddf1518362b",
      "34f77d9c0e8d4a6aa181edebaf95dd5d",
      "8fcc361b676f40b4961fe5df7cf1b7d1",
      "1c7558ea07694b279c16928398ee87f1",
      "232d6eb9a2274724ae3846d12eafef88",
      "5362704bf0a943f7aa846c8b45fbe9b7",
      "142683e9e2a5487bb92c40716231e835",
      "b7128d81e3c84c2dbebb24f7c4c82b80",
      "60658829a5d942529b087c06ad86ae0a",
      "6b35c50241a449168b624170840c799b",
      "72bbc394823e4d54a51c799e266368ba",
      "3d0c3740346a423cbafaea54982409a7",
      "c9e5d0f4867a4fbaaf8e9c84bc1f9e81",
      "6ef55d42a3234fa1986e45383ed9bf92",
      "2c6fb3eab7a84e2f9382b47fee5c0ec6",
      "d9e7112252094f259cf8f1f552610f30",
      "12fbe8cd653c497fb9bb5a1fc28d84ca",
      "99e3ef4d9aba4880ab59f9fa12209080",
      "7b168526e6dc4a40bbea861562714f75",
      "656a9ab27f844596a1de731588e05adc",
      "8a45a1a1973a4b70a05b7d92bfcdf24b",
      "237f0d2216c8445781f436272416c3ae",
      "a50de349a5e44713a1c44b30ef35e32f",
      "03234e888924428cba43c2e2a4920592",
      "9e30af5708374765846c88d9e60e7404",
      "dacad693134b4c09ad537a4e7aa4f6f3",
      "8096cde6fc624092b630e5d1f29d3acf",
      "ab7c5d2e33cb447fa7fb049ccacabe1e",
      "1ab9ccde456d48c8aa628621882933c9",
      "bbbeb718e5884556a4585955c70a5f57",
      "103ae64c432b4e98bf5f7884208845b0",
      "9600f3050dd3401388712d47014c517f",
      "4b5e8dbc722b417694a1461b30468774",
      "b9103099b4454f6fb723075641f2b0fa",
      "41667ba4da624d05992df4e6ab1c15a9",
      "6cf76bb7a22d4d59ab8e4fbf822df856",
      "a8515e57442c4d8c83b20aeade32df01"
     ]
    },
    "id": "90e4ebff",
    "outputId": "6613dd10-5a30-4798-9747-6d972aa2043f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beecc1d94654e7dbc7246b5880b3972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f342b86fe040bdb359901814c4000d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b77444da2c4bdd8b162bf5af0b43bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232d6eb9a2274724ae3846d12eafef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e7112252094f259cf8f1f552610f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8096cde6fc624092b630e5d1f29d3acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.36399322748184204],\n",
       " 'recall': [0.17047250270843506],\n",
       " 'f1': [0.2666224539279938],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.52.4)-rescaled'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_scorer.compute(\n",
    "    predictions=[ai_generated_summary],\n",
    "    references=[human_generated_summary],\n",
    "    lang=\"en\",\n",
    "    rescale_with_baseline=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
